var documenterSearchIndex = {"docs":
[{"location":"api/functional/","page":"Functional","title":"Functional","text":"CurrentModule = Lux","category":"page"},{"location":"api/functional/#Functional-Layers","page":"Functional","title":"Functional Layers","text":"","category":"section"},{"location":"api/functional/","page":"Functional","title":"Functional","text":"note: Note\nThese functions expose the backend of Lux.jl. In the long-term we plan to move these into NNlib","category":"page"},{"location":"api/functional/","page":"Functional","title":"Functional","text":"Lux.dropout\nLux.normalization","category":"page"},{"location":"api/functional/#Lux.dropout","page":"Functional","title":"Lux.dropout","text":"dropout(rng::AbstractRNG, x, prob, dims, ::Val{training})\ndropout(rng::AbstractRNG, x, mask, prob, dims, t::Val{training}, ::Val{update_mask})\n\nIf training then dropout is applied on x with probability prob along dims. If mask is passed it is used if update_mask is false. If update_mask is true then the mask is generated and used.\n\n\n\n\n\n","category":"function"},{"location":"api/functional/#Lux.normalization","page":"Functional","title":"Lux.normalization","text":"normalization(x, running_mean, running_var, scale, bias, activation, reduce_dims, ::Val{training}, momentum, epsilon)\n\nPerforms BatchNorm/GroupNorm/InstanceNorm based on input configuration\n\nnote: Note\nDetailed docs are WIP\n\n\n\n\n\n","category":"function"},{"location":"introduction/ecosystem/#Extended-Deep-Learning-Ecosystem","page":"Ecosystem","title":"Extended Deep Learning Ecosystem","text":"","category":"section"},{"location":"introduction/ecosystem/","page":"Ecosystem","title":"Ecosystem","text":"As you might have noticed we don't do much apart from Neural Networks. All other parts of the DL training/evaluation pipeline should be offloaded to:","category":"page"},{"location":"introduction/ecosystem/","page":"Ecosystem","title":"Ecosystem","text":"Data Manipulation/Loading – Augmentor.jl, DataLoaders.jl, Images.jl, DataAugmentation.jl\nOptimisation – Optimisers.jl, ParameterSchedulers.jl\nAutomatic Differentiation – Zygote.jl (Default AD), Enzyme.jl (Experimental Support)\nParameter Manipulation – Functors.jl\nModel Checkpointing – Serialization.jl\nActivation Functions / Common Neural Network Primitives – NNlib.jl\nDistributed DataParallel Training – FluxMPI.jl\nTraining Visualization – Wandb.jl, TensorBoardLogger.jl","category":"page"},{"location":"design/core/#Adding-New-Functionality/Layers","page":"Add new functionality to Lux","title":"Adding New Functionality/Layers","text":"","category":"section"},{"location":"design/core/","page":"Add new functionality to Lux","title":"Add new functionality to Lux","text":"For Style we try to follow SciMLStyle. The only reason we don't have a badge yet, is we haven't yet updated the package to followed all the guidelines. Here, I am documenting some additional guidelines we enforce:","category":"page"},{"location":"design/core/#Mutability","page":"Add new functionality to Lux","title":"Mutability","text":"","category":"section"},{"location":"design/core/","page":"Add new functionality to Lux","title":"Add new functionality to Lux","text":"See https://github.com/SciML/SciMLStyle#out-of-place-and-immutability-is-preferred-when-sufficient-performant for reference. This is strictly enforced, i.e. all layers/functions provided as part of the external API must be pure functions, even if they come with a performance penalty.","category":"page"},{"location":"design/core/#Branching-–-Generated-Functions","page":"Add new functionality to Lux","title":"Branching – Generated Functions","text":"","category":"section"},{"location":"design/core/","page":"Add new functionality to Lux","title":"Add new functionality to Lux","text":"Zygote doesn't like branches in code. Like it or not, we are stuck with it for the near future. Even if julia is able to optimize branches away, Zygote will most certainly throw away those optimizations (these can be tested via Zygote.@code_ir).","category":"page"},{"location":"design/core/#Writing-efficient-non-branching-code-to-make-Zygote-happy","page":"Add new functionality to Lux","title":"Writing efficient non-branching code to make Zygote happy","text":"","category":"section"},{"location":"design/core/","page":"Add new functionality to Lux","title":"Add new functionality to Lux","text":"Rely on @generated functions to remove most runtime branching. Certain examples:\nLayers behaving differently during training and inference – we know at compile-time whether a layer is being run in training/inference mode via istraining(st).\nComposite Layers relying on a variable number of internal layers – Again we know the length of the number of internal layers at compile time. Hence we can manually unroll the loops. See Parallel, Chain, etc.\nPass around Val in state. Flux.jl sets training to be (:auto, true, false). Hence, which branch will be evaluated, will have to be determined at runtime time (bad). Instead if we pass Val(true), we will be able to specialize functions directly based on true, false, etc. ensuring there is no runtime cost for these operations. See BatchNorm, Dropout, etc.","category":"page"},{"location":"api/core/","page":"Core","title":"Core","text":"CurrentModule = Lux","category":"page"},{"location":"api/core/#General","page":"Core","title":"General","text":"","category":"section"},{"location":"api/core/","page":"Core","title":"Core","text":"Lux.apply\nLux.setup","category":"page"},{"location":"api/core/#Lux.apply","page":"Core","title":"Lux.apply","text":"apply(model::AbstractExplicitLayer, x, ps::Union{ComponentArray,NamedTuple}, st::NamedTuple)\n\nSimply calls model(x, ps, st)\n\n\n\n\n\n","category":"function"},{"location":"api/core/#Lux.setup","page":"Core","title":"Lux.setup","text":"setup(rng::AbstractRNG, l::AbstractExplicitLayer)\n\nShorthand for getting the parameters and states of the layer l. Is equivalent to (initialparameters(rng, l), initialstates(rng, l)).\n\n\n\n\n\n","category":"function"},{"location":"api/core/#Parameters","page":"Core","title":"Parameters","text":"","category":"section"},{"location":"api/core/","page":"Core","title":"Core","text":"Lux.initialparameters\nLux.parameterlength","category":"page"},{"location":"api/core/#Lux.initialparameters","page":"Core","title":"Lux.initialparameters","text":"initialparameters(rng::AbstractRNG, l)\n\nGenerate the initial parameters of the layer l.\n\n\n\n\n\n","category":"function"},{"location":"api/core/#Lux.parameterlength","page":"Core","title":"Lux.parameterlength","text":"parameterlength(l)\n\nReturn the total number of parameters of the layer l.\n\n\n\n\n\n","category":"function"},{"location":"api/core/#States","page":"Core","title":"States","text":"","category":"section"},{"location":"api/core/","page":"Core","title":"Core","text":"Lux.initialstates\nLux.statelength\nLux.testmode\nLux.trainmode\nLux.update_state","category":"page"},{"location":"api/core/#Lux.initialstates","page":"Core","title":"Lux.initialstates","text":"initialstates(rng::AbstractRNG, l)\n\nGenerate the initial states of the layer l.\n\n\n\n\n\n","category":"function"},{"location":"api/core/#Lux.statelength","page":"Core","title":"Lux.statelength","text":"statelength(l)\n\nReturn the total number of states of the layer l.\n\n\n\n\n\n","category":"function"},{"location":"api/core/#Lux.testmode","page":"Core","title":"Lux.testmode","text":"testmode(st::NamedTuple, mode::Bool=true)\n\nMake all occurances of training in state st !mode\n\n\n\n\n\n","category":"function"},{"location":"api/core/#Lux.trainmode","page":"Core","title":"Lux.trainmode","text":"trainmode(x::Any, mode::Bool=true)\n\nMake all occurances of training in state st mode\n\n\n\n\n\n","category":"function"},{"location":"api/core/#Lux.update_state","page":"Core","title":"Lux.update_state","text":"update_state(st::NamedTuple, key::Symbol, value; layer_check=_default_layer_check(key))\n\nRecursively update all occurances of the key in the state st with the value.\n\n\n\n\n\n","category":"function"},{"location":"api/core/#Index","page":"Core","title":"Index","text":"","category":"section"},{"location":"api/core/","page":"Core","title":"Core","text":"Pages = [\"core.md\"]","category":"page"},{"location":"api/utilities/","page":"Utilities","title":"Utilities","text":"CurrentModule = Lux","category":"page"},{"location":"api/utilities/#Data-Transfer","page":"Utilities","title":"Data Transfer","text":"","category":"section"},{"location":"api/utilities/","page":"Utilities","title":"Utilities","text":"Lux.cpu\nLux.gpu","category":"page"},{"location":"api/utilities/#Lux.cpu","page":"Utilities","title":"Lux.cpu","text":"cpu(x)\n\nTransfer x to CPU\n\n\n\n\n\n","category":"function"},{"location":"api/utilities/#Lux.gpu","page":"Utilities","title":"Lux.gpu","text":"gpu(x)\n\nTransfer x to GPU\n\n\n\n\n\n","category":"function"},{"location":"api/utilities/#Initialization","page":"Utilities","title":"Initialization","text":"","category":"section"},{"location":"api/utilities/","page":"Utilities","title":"Utilities","text":"Lux.glorot_normal\nLux.glorot_uniform\nLux.ones32\nLux.zeros32","category":"page"},{"location":"api/utilities/#Lux.glorot_normal","page":"Utilities","title":"Lux.glorot_normal","text":"glorot_normal(rng::AbstractRNG, size...; gain = 1)\n\nReturn an Array{Float32} of the given size containing random numbers drawn from a normal distribution with standard deviation gain * sqrt(2 / (fan_in + fan_out)). This method is described in [1] and also known as Xavier initialization.\n\nReferences\n\n[1] Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" Proceedings of the thirteenth international conference on artificial intelligence and statistics. 2010.\n\n\n\n\n\n","category":"function"},{"location":"api/utilities/#Lux.glorot_uniform","page":"Utilities","title":"Lux.glorot_uniform","text":"glorot_uniform(rng::AbstractRNG, size...; gain = 1)\n\nReturn an Array{Float32} of the given size containing random numbers drawn from a uniform distribution on the interval -x x, where x = gain * sqrt(6 / (fan_in + fan_out)). This method is described in [1] and also known as Xavier initialization.\n\nReferences\n\n[1] Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" Proceedings of the thirteenth international conference on artificial intelligence and statistics. 2010.\n\n\n\n\n\n","category":"function"},{"location":"api/utilities/#Lux.ones32","page":"Utilities","title":"Lux.ones32","text":"ones32(rng::AbstractRNG, size...) = ones(Float32, size...)\n\nReturn an Array{Float32} of ones of the given size. (rng is ignored)\n\n\n\n\n\n","category":"function"},{"location":"api/utilities/#Lux.zeros32","page":"Utilities","title":"Lux.zeros32","text":"zeros32(rng::AbstractRNG, size...) = zeros(Float32, size...)\n\nReturn an Array{Float32} of zeros of the given size. (rng is ignored)\n\n\n\n\n\n","category":"function"},{"location":"api/utilities/#Miscellaneous-Utilities","page":"Utilities","title":"Miscellaneous Utilities","text":"","category":"section"},{"location":"api/utilities/","page":"Utilities","title":"Utilities","text":"Lux.applyactivation\nLux.elementwise_add\nLux.elementwise_mul","category":"page"},{"location":"api/utilities/#Lux.applyactivation","page":"Utilities","title":"Lux.applyactivation","text":"applyactivation(f::Function, x::AbstractArray)\n\nApply the function f on x elementwise, i.e. f.(x). Dispatches to CUDNN if possible.\n\n\n\n\n\n","category":"function"},{"location":"api/utilities/#Lux.elementwise_add","page":"Utilities","title":"Lux.elementwise_add","text":"elementwise_add(x, y)\n\nComputes x .+ y. Dispatches to CUDNN if possible\n\n\n\n\n\n","category":"function"},{"location":"api/utilities/#Lux.elementwise_mul","page":"Utilities","title":"Lux.elementwise_mul","text":"elementwise_mul(x, y)\n\nComputes x .* y. Dispatches to CUDNN if possible\n\n\n\n\n\n","category":"function"},{"location":"api/utilities/#RNN-Utilities","page":"Utilities","title":"RNN Utilities","text":"","category":"section"},{"location":"api/utilities/","page":"Utilities","title":"Utilities","text":"Lux.multigate","category":"page"},{"location":"api/utilities/#Lux.multigate","page":"Utilities","title":"Lux.multigate","text":"multigate(x::AbstractArray, ::Val{N})\n\nSplit up x into N equally sized chunks (along dimension 1).\n\n\n\n\n\n","category":"function"},{"location":"api/utilities/#Index","page":"Utilities","title":"Index","text":"","category":"section"},{"location":"api/utilities/","page":"Utilities","title":"Utilities","text":"Pages = [\"utilities.md\"]","category":"page"},{"location":"design/recurrent/#Recurrent-Neural-Networks","page":"Recurrent Neural Networks","title":"Recurrent Neural Networks","text":"","category":"section"},{"location":"design/recurrent/#Cell-Implementations","page":"Recurrent Neural Networks","title":"Cell Implementations","text":"","category":"section"},{"location":"design/recurrent/#Explicit-Management-on-End-User-Side","page":"Recurrent Neural Networks","title":"Explicit Management on End-User Side","text":"","category":"section"},{"location":"design/recurrent/","page":"Recurrent Neural Networks","title":"Recurrent Neural Networks","text":"note: Note\nWe currently use this implementation","category":"page"},{"location":"design/recurrent/","page":"Recurrent Neural Networks","title":"Recurrent Neural Networks","text":"User is responsible for managing the memory and hidden states.","category":"page"},{"location":"design/recurrent/#Pros","page":"Recurrent Neural Networks","title":"Pros","text":"","category":"section"},{"location":"design/recurrent/","page":"Recurrent Neural Networks","title":"Recurrent Neural Networks","text":"Simple Design and Implementation\nHard for the User to mess up, i.e. there is no explicit requirement to call things like Flux.reset!\nIn the first call user passes the input\nIn the subsequent calls, the user passes a tuple containing the input, hidden_state and memory (if needed)","category":"page"},{"location":"design/recurrent/#Cons","page":"Recurrent Neural Networks","title":"Cons","text":"","category":"section"},{"location":"design/recurrent/","page":"Recurrent Neural Networks","title":"Recurrent Neural Networks","text":"Requires more explicit management from the user which might make it harder to use.\nCurrently the call order convention is not enforced which could lead to sneaky errors. (Implementing a check is quite trivial if we store a call counter in the model state)","category":"page"},{"location":"design/recurrent/#Store-Hidden-State-and-Memory-in-Model-State","page":"Recurrent Neural Networks","title":"Store Hidden State and Memory in Model State","text":"","category":"section"},{"location":"design/recurrent/","page":"Recurrent Neural Networks","title":"Recurrent Neural Networks","text":"Storing the memory and hidden state in st would allow user to just pass x without varying how calls are made at different timesteps","category":"page"},{"location":"design/recurrent/#Pros-2","page":"Recurrent Neural Networks","title":"Pros","text":"","category":"section"},{"location":"design/recurrent/","page":"Recurrent Neural Networks","title":"Recurrent Neural Networks","text":"Easier for the end-user","category":"page"},{"location":"design/recurrent/#Cons-2","page":"Recurrent Neural Networks","title":"Cons","text":"","category":"section"},{"location":"design/recurrent/","page":"Recurrent Neural Networks","title":"Recurrent Neural Networks","text":"reseting the hidden-state and memory is slightly tricky.\nOne way would be to store a initial_hidden_state and initial_memory in the state alongside the hidden_state and memory","category":"page"},{"location":"design/recurrent/#RNN-Blocks","page":"Recurrent Neural Networks","title":"RNN Blocks","text":"","category":"section"},{"location":"design/recurrent/","page":"Recurrent Neural Networks","title":"Recurrent Neural Networks","text":"note: Note\nThis is currently unimplemented","category":"page"},{"location":"design/recurrent/","page":"Recurrent Neural Networks","title":"Recurrent Neural Networks","text":"An example implementation would be","category":"page"},{"location":"design/recurrent/","page":"Recurrent Neural Networks","title":"Recurrent Neural Networks","text":"function (l::LSTM)(x::AbstractArray{T,3}, ps::NamedTuple, st::NamedTuple) where {T}\n    (h, c), st = s.lstm_cell(view(x, :, 1, :), ps, st)\n    for i in 1:size(x, 2)\n        (h, c), st = s.lstm_cell((view(x, :, i, :), h, c), ps, st)\n    end\n    return h, st\nend","category":"page"},{"location":"design/recurrent/","page":"Recurrent Neural Networks","title":"Recurrent Neural Networks","text":"We enforce the inputs to be of the format in_dims × sequence_length × batch_size","category":"page"},{"location":"examples/generated/beginner/SimpleRNN/main/","page":"Training a Simple LSTM","title":"Training a Simple LSTM","text":"EditURL = \"https://github.com/avik-pal/Lux.jl/blob/main/examples/SimpleRNN/main.jl\"","category":"page"},{"location":"examples/generated/beginner/SimpleRNN/main/#Training-a-Simple-LSTM","page":"Training a Simple LSTM","title":"Training a Simple LSTM","text":"","category":"section"},{"location":"examples/generated/beginner/SimpleRNN/main/","page":"Training a Simple LSTM","title":"Training a Simple LSTM","text":"In this tutorial we will go over using a recurrent neural network to classify clockwise and anticlockwise spirals. By the end of this tutorial you will be able to:","category":"page"},{"location":"examples/generated/beginner/SimpleRNN/main/","page":"Training a Simple LSTM","title":"Training a Simple LSTM","text":"Create custom Lux models\nBecome familiar with the Lux recurrent neural network API\nTraining using Optimisers.jl and Zygote.jl","category":"page"},{"location":"examples/generated/beginner/SimpleRNN/main/#Package-Imports","page":"Training a Simple LSTM","title":"Package Imports","text":"","category":"section"},{"location":"examples/generated/beginner/SimpleRNN/main/","page":"Training a Simple LSTM","title":"Training a Simple LSTM","text":"using Lux\nusing Pkg #hide\nPkg.activate(joinpath(dirname(pathof(Lux)), \"..\", \"examples\")) #hide\nusing MLUtils, Optimisers, Zygote, NNlib, Random, Statistics","category":"page"},{"location":"examples/generated/beginner/SimpleRNN/main/#Dataset","page":"Training a Simple LSTM","title":"Dataset","text":"","category":"section"},{"location":"examples/generated/beginner/SimpleRNN/main/","page":"Training a Simple LSTM","title":"Training a Simple LSTM","text":"We will use MLUtils to generate 500 (noisy) clockwise and 500 (noisy) anticlockwise spirals. Using this data we will create a MLUtils.DataLoader. Our dataloader will give us sequences of size 2 × seqlen × batchsize and we need to predict a binary value whether the sequence is clockwise or anticlockwise","category":"page"},{"location":"examples/generated/beginner/SimpleRNN/main/","page":"Training a Simple LSTM","title":"Training a Simple LSTM","text":"function get_dataloaders(; dataset_size=1000, sequence_length=50)\n    # Create the spirals\n    data = [MLUtils.Datasets.make_spiral(sequence_length) for _ in 1:dataset_size]\n    # Get the labels\n    labels = vcat(repeat([0.0f0], dataset_size ÷ 2), repeat([1.0f0], dataset_size ÷ 2))\n    clockwise_spirals = [reshape(d[1][:, 1:sequence_length], :, sequence_length, 1)\n                         for d in data[1:(dataset_size ÷ 2)]]\n    anticlockwise_spirals = [reshape(d[1][:, (sequence_length + 1):end], :, sequence_length,\n                                     1) for d in data[((dataset_size ÷ 2) + 1):end]]\n    x_data = Float32.(cat(clockwise_spirals..., anticlockwise_spirals...; dims=3))\n    # Split the dataset\n    (x_train, y_train), (x_val, y_val) = splitobs((x_data, labels); at=0.8,\n                                                  shuffle=true)\n    # Create DataLoaders\n    return (\n            # Use DataLoader to automatically minibatch and shuffle the data\n            DataLoader(collect.((x_train, y_train)); batchsize=128, shuffle=true),\n            # Don't shuffle the validation data\n            DataLoader(collect.((x_val, y_val)); batchsize=128, shuffle=false))\nend","category":"page"},{"location":"examples/generated/beginner/SimpleRNN/main/#Creating-a-Classifier","page":"Training a Simple LSTM","title":"Creating a Classifier","text":"","category":"section"},{"location":"examples/generated/beginner/SimpleRNN/main/","page":"Training a Simple LSTM","title":"Training a Simple LSTM","text":"We will be extending the Lux.AbstractExplicitContainerLayer type for our custom model since it will contain a lstm block and a classifier head.","category":"page"},{"location":"examples/generated/beginner/SimpleRNN/main/","page":"Training a Simple LSTM","title":"Training a Simple LSTM","text":"We pass the fieldnames lstm_cell and classifier to the type to ensure that the parameters and states are automatically populated and we don't have to define Lux.initialparameters and Lux.initialstates.","category":"page"},{"location":"examples/generated/beginner/SimpleRNN/main/","page":"Training a Simple LSTM","title":"Training a Simple LSTM","text":"struct SpiralClassifier{L, C} <:\n       Lux.AbstractExplicitContainerLayer{(:lstm_cell, :classifier)}\n    lstm_cell::L\n    classifier::C\nend","category":"page"},{"location":"examples/generated/beginner/SimpleRNN/main/","page":"Training a Simple LSTM","title":"Training a Simple LSTM","text":"We won't define the model from scratch but rather use the Lux.LSTMCell and Lux.Dense","category":"page"},{"location":"examples/generated/beginner/SimpleRNN/main/","page":"Training a Simple LSTM","title":"Training a Simple LSTM","text":"function SpiralClassifier(in_dims, hidden_dims, out_dims)\n    return SpiralClassifier(LSTMCell(in_dims => hidden_dims),\n                            Dense(hidden_dims => out_dims, sigmoid))\nend","category":"page"},{"location":"examples/generated/beginner/SimpleRNN/main/","page":"Training a Simple LSTM","title":"Training a Simple LSTM","text":"Now we need to define the behavior of the Classifier when it is invoked","category":"page"},{"location":"examples/generated/beginner/SimpleRNN/main/","page":"Training a Simple LSTM","title":"Training a Simple LSTM","text":"function (s::SpiralClassifier)(x::AbstractArray{T, 3}, ps::NamedTuple,\n                               st::NamedTuple) where {T}\n    # First we will have to run the sequence through the LSTM Cell\n    # The first call to LSTM Cell will create the initial hidden state\n    # See that the parameters and states are automatically populated into a field called `lstm_cell`\n    # We use `view(x, :, 1, :)` to get the first element in the sequence without copying it\n    (h, c), st_lstm = s.lstm_cell(view(x, :, 1, :), ps.lstm_cell, st.lstm_cell)\n    # Now that we have the hidden state we will pass the input and hidden state jointly\n    for i in 1:size(x, 2)\n        (h, c), st_lstm = s.lstm_cell((view(x, :, i, :), h, c), ps.lstm_cell, st_lstm)\n    end\n    # After running through the sequence we will pass the output through the classifier\n    y, st_classifier = s.classifier(h, ps.classifier, st.classifier)\n    # Finally remember to create the updated state\n    st = merge(st, (classifier=st_classifier, lstm_cell=st_lstm))\n    return vec(y), st\nend","category":"page"},{"location":"examples/generated/beginner/SimpleRNN/main/#Defining-Accuracy,-Loss-and-Optimiser","page":"Training a Simple LSTM","title":"Defining Accuracy, Loss and Optimiser","text":"","category":"section"},{"location":"examples/generated/beginner/SimpleRNN/main/","page":"Training a Simple LSTM","title":"Training a Simple LSTM","text":"Now let's define the binarycrossentropy loss. Typically it is recommended to use logitbinarycrossentropy since it is more numerically stable, but for the sake of simplicity we will use binarycrossentropy","category":"page"},{"location":"examples/generated/beginner/SimpleRNN/main/","page":"Training a Simple LSTM","title":"Training a Simple LSTM","text":"function xlogy(x, y)\n    result = x * log(y)\n    return ifelse(iszero(x), zero(result), result)\nend\n\nfunction binarycrossentropy(y_pred, y_true)\n    y_pred = y_pred .+ eps(eltype(y_pred))\n    return mean(@. -xlogy(y_true, y_pred) - xlogy(1 - y_true, 1 - y_pred))\nend\n\nfunction compute_loss(x, y, model, ps, st)\n    y_pred, st = model(x, ps, st)\n    return binarycrossentropy(y_pred, y), y_pred, st\nend\n\nmatches(y_pred, y_true) = sum((y_pred .> 0.5) .== y_true)\naccuracy(y_pred, y_true) = matches(y_pred, y_true) / length(y_pred)","category":"page"},{"location":"examples/generated/beginner/SimpleRNN/main/","page":"Training a Simple LSTM","title":"Training a Simple LSTM","text":"Finally lets create an optimiser given the model parameters","category":"page"},{"location":"examples/generated/beginner/SimpleRNN/main/","page":"Training a Simple LSTM","title":"Training a Simple LSTM","text":"function create_optimiser(ps)\n    opt = Optimisers.ADAM(0.01f0)\n    return Optimisers.setup(opt, ps)\nend","category":"page"},{"location":"examples/generated/beginner/SimpleRNN/main/#Training-the-Model","page":"Training a Simple LSTM","title":"Training the Model","text":"","category":"section"},{"location":"examples/generated/beginner/SimpleRNN/main/","page":"Training a Simple LSTM","title":"Training a Simple LSTM","text":"function main()\n    # Get the dataloaders\n    (train_loader, val_loader) = get_dataloaders()\n\n    # Create the model\n    model = SpiralClassifier(2, 8, 1)\n    rng = Random.default_rng()\n    Random.seed!(rng, 0)\n    ps, st = Lux.setup(rng, model)\n\n    # Create the optimiser\n    opt_state = create_optimiser(ps)\n\n    for epoch in 1:25\n        # Train the model\n        for (x, y) in train_loader\n            (loss, y_pred, st), back = pullback(p -> compute_loss(x, y, model, p, st), ps)\n            gs = back((one(loss), nothing, nothing))[1]\n            opt_state, ps = Optimisers.update(opt_state, ps, gs)\n\n            println(\"Epoch [$epoch]: Loss $loss\")\n        end\n\n        # Validate the model\n        st_ = Lux.testmode(st)\n        for (x, y) in val_loader\n            (loss, y_pred, st_) = compute_loss(x, y, model, ps, st_)\n            acc = accuracy(y_pred, y)\n            println(\"Validation: Loss $loss Accuracy $acc\")\n        end\n    end\nend\n\nmain()","category":"page"},{"location":"examples/generated/beginner/SimpleRNN/main/","page":"Training a Simple LSTM","title":"Training a Simple LSTM","text":"","category":"page"},{"location":"examples/generated/beginner/SimpleRNN/main/","page":"Training a Simple LSTM","title":"Training a Simple LSTM","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/generated/intermediate/NeuralODE/main/","page":"MNIST NeuralODE Classification","title":"MNIST NeuralODE Classification","text":"EditURL = \"https://github.com/avik-pal/Lux.jl/blob/main/examples/NeuralODE/main.jl\"","category":"page"},{"location":"examples/generated/intermediate/NeuralODE/main/#MNIST-Classification-using-Neural-ODEs","page":"MNIST NeuralODE Classification","title":"MNIST Classification using Neural ODEs","text":"","category":"section"},{"location":"examples/generated/intermediate/NeuralODE/main/#Package-Imports","page":"MNIST NeuralODE Classification","title":"Package Imports","text":"","category":"section"},{"location":"examples/generated/intermediate/NeuralODE/main/","page":"MNIST NeuralODE Classification","title":"MNIST NeuralODE Classification","text":"using Lux\nusing Pkg #hide\nPkg.activate(joinpath(dirname(pathof(Lux)), \"..\", \"examples\")) #hide\nusing ComponentArrays, CUDA, DiffEqSensitivity, NNlib, Optimisers, OrdinaryDiffEq, Random,\n      Statistics, Zygote\nimport MLDatasets: MNIST\nimport MLDataUtils: convertlabel, LabelEnc\nimport MLUtils: DataLoader, splitobs\nCUDA.allowscalar(false)","category":"page"},{"location":"examples/generated/intermediate/NeuralODE/main/#Loading-MNIST","page":"MNIST NeuralODE Classification","title":"Loading MNIST","text":"","category":"section"},{"location":"examples/generated/intermediate/NeuralODE/main/","page":"MNIST NeuralODE Classification","title":"MNIST NeuralODE Classification","text":"# Use MLDataUtils LabelEnc for natural onehot conversion\nfunction onehot(labels_raw)\n    convertlabel(LabelEnc.OneOfK, labels_raw, LabelEnc.NativeLabels(collect(0:9)))\nend\n\nfunction loadmnist(batchsize, train_split)\n    # Load MNIST: Only 1500 for demonstration purposes\n    N = 1500\n    imgs = MNIST.traintensor(1:N)\n    labels_raw = MNIST.trainlabels(1:N)\n\n    # Process images into (H,W,C,BS) batches\n    x_data = Float32.(reshape(imgs, size(imgs, 1), size(imgs, 2), 1, size(imgs, 3)))\n    y_data = onehot(labels_raw)\n    (x_train, y_train), (x_test, y_test) = splitobs((x_data, y_data); at=train_split)\n\n    return (\n            # Use DataLoader to automatically minibatch and shuffle the data\n            DataLoader(collect.((x_train, y_train)); batchsize=batchsize, shuffle=true),\n            # Don't shuffle the test data\n            DataLoader(collect.((x_test, y_test)); batchsize=batchsize, shuffle=false))\nend","category":"page"},{"location":"examples/generated/intermediate/NeuralODE/main/#Define-the-Neural-ODE-Layer","page":"MNIST NeuralODE Classification","title":"Define the Neural ODE Layer","text":"","category":"section"},{"location":"examples/generated/intermediate/NeuralODE/main/","page":"MNIST NeuralODE Classification","title":"MNIST NeuralODE Classification","text":"The NeuralODE is a ContainerLayer. It stores a model and the parameters and states of the NeuralODE is same as that of the underlying model.","category":"page"},{"location":"examples/generated/intermediate/NeuralODE/main/","page":"MNIST NeuralODE Classification","title":"MNIST NeuralODE Classification","text":"struct NeuralODE{M <: Lux.AbstractExplicitLayer, So, Se, T, K} <:\n       Lux.AbstractExplicitContainerLayer{(:model,)}\n    model::M\n    solver::So\n    sensealg::Se\n    tspan::T\n    kwargs::K\nend\n\nfunction NeuralODE(model::Lux.AbstractExplicitLayer;\n                   solver=Tsit5(),\n                   sensealg=InterpolatingAdjoint(; autojacvec=ZygoteVJP()),\n                   tspan=(0.0f0, 1.0f0),\n                   kwargs...)\n    return NeuralODE(model, solver, sensealg, tspan, kwargs)\nend\n\nfunction (n::NeuralODE)(x, ps, st)\n    function dudt(u, p, t)\n        u_, st = n.model(u, p, st)\n        return u_\n    end\n    prob = ODEProblem{false}(ODEFunction{false}(dudt), x, n.tspan, ps)\n    return solve(prob, n.solver; sensealg=n.sensealg, n.kwargs...), st\nend\n\nfunction diffeqsol_to_array(x::ODESolution{T, N, <:AbstractVector{<:CuArray}}) where {T, N}\n    dropdims(gpu(x); dims=3)\nend\ndiffeqsol_to_array(x::ODESolution) = dropdims(Array(x); dims=3)","category":"page"},{"location":"examples/generated/intermediate/NeuralODE/main/#Create-and-Initialize-the-Neural-ODE-Layer","page":"MNIST NeuralODE Classification","title":"Create and Initialize the Neural ODE Layer","text":"","category":"section"},{"location":"examples/generated/intermediate/NeuralODE/main/","page":"MNIST NeuralODE Classification","title":"MNIST NeuralODE Classification","text":"function create_model()\n    # Construct the Neural ODE Model\n    model = Chain(FlattenLayer(),\n                  Dense(784, 20, tanh),\n                  NeuralODE(Chain(Dense(20, 10, tanh), Dense(10, 10, tanh),\n                                  Dense(10, 20, tanh));\n                            save_everystep=false,\n                            reltol=1.0f-3,\n                            abstol=1.0f-3,\n                            save_start=false),\n                  diffeqsol_to_array,\n                  Dense(20, 10))\n\n    rng = Random.default_rng()\n    Random.seed!(rng, 0)\n\n    ps, st = Lux.setup(rng, model)\n    ps = ComponentArray(ps) |> gpu\n    st = st |> gpu\n\n    return model, ps, st\nend","category":"page"},{"location":"examples/generated/intermediate/NeuralODE/main/#Define-Utility-Functions","page":"MNIST NeuralODE Classification","title":"Define Utility Functions","text":"","category":"section"},{"location":"examples/generated/intermediate/NeuralODE/main/","page":"MNIST NeuralODE Classification","title":"MNIST NeuralODE Classification","text":"get_class(x) = argmax.(eachcol(x))\n\nlogitcrossentropy(ŷ, y) = mean(-sum(y .* logsoftmax(ŷ); dims=1))\n\nfunction loss(x, y, model, ps, st)\n    ŷ, st = model(x, ps, st)\n    return logitcrossentropy(ŷ, y), st\nend\n\nfunction accuracy(model, ps, st, dataloader)\n    total_correct, total = 0, 0\n    st = Lux.testmode(st)\n    iterator = CUDA.functional() ? CuIterator(dataloader) : dataloader\n    for (x, y) in iterator\n        target_class = get_class(cpu(y))\n        predicted_class = get_class(cpu(model(x, ps, st)[1]))\n        total_correct += sum(target_class .== predicted_class)\n        total += length(target_class)\n    end\n    return total_correct / total\nend","category":"page"},{"location":"examples/generated/intermediate/NeuralODE/main/#Training","page":"MNIST NeuralODE Classification","title":"Training","text":"","category":"section"},{"location":"examples/generated/intermediate/NeuralODE/main/","page":"MNIST NeuralODE Classification","title":"MNIST NeuralODE Classification","text":"function train()\n    model, ps, st = create_model()\n\n    # Training\n    train_dataloader, test_dataloader = loadmnist(128, 0.9)\n\n    opt = Optimisers.ADAM(0.001f0)\n    st_opt = Optimisers.setup(opt, ps)\n\n    ### Warmup the Model\n    img, lab = gpu(train_dataloader.data[1][:, :, :, 1:1]),\n               gpu(train_dataloader.data[2][:, 1:1])\n    loss(img, lab, model, ps, st)\n    (l, _), back = pullback(p -> loss(img, lab, model, p, st), ps)\n    back((one(l), nothing))\n\n    ### Lets train the model\n    nepochs = 9\n    for epoch in 1:nepochs\n        stime = time()\n        iterator = CUDA.functional() ? CuIterator(train_dataloader) : train_dataloader\n        for (x, y) in iterator\n            (l, _), back = pullback(p -> loss(x, y, model, p, st), ps)\n            gs = back((one(l), nothing))[1]\n            st_opt, ps = Optimisers.update(st_opt, ps, gs)\n        end\n        ttime = time() - stime\n\n        println(\"[$epoch/$nepochs] \\t Time $(round(ttime; digits=2))s \\t Training Accuracy: \" *\n                \"$(round(accuracy(model, ps, st, train_dataloader) * 100; digits=2))% \\t \" *\n                \"Test Accuracy: $(round(accuracy(model, ps, st, test_dataloader) * 100; digits=2))%\")\n    end\nend\n\ntrain()","category":"page"},{"location":"examples/generated/intermediate/NeuralODE/main/","page":"MNIST NeuralODE Classification","title":"MNIST NeuralODE Classification","text":"","category":"page"},{"location":"examples/generated/intermediate/NeuralODE/main/","page":"MNIST NeuralODE Classification","title":"MNIST NeuralODE Classification","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"EditURL = \"https://github.com/avik-pal/Lux.jl/blob/main/examples/Basics/main.jl\"","category":"page"},{"location":"examples/generated/beginner/Basics/main/#Julia-and-Lux-for-the-Uninitiated","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"","category":"section"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"This is a quick intro to Lux loosely based on:","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"PyTorch's tutorial.\nFlux's tutorial.\nFlax's tutorial.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"It introduces basic Julia programming, as well Zygote, a source-to-source automatic differentiation (AD) framework in Julia. We'll use these tools to build a very simple neural network. Let's start with importing Lux.jl","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"using Lux, Random\nusing Pkg #hide\nPkg.activate(joinpath(dirname(pathof(Lux)), \"..\", \"examples\")) #hide","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"Now let us control the randomness in our code using proper Pseudo Random Number Generator (PRNG)","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"rng = Random.default_rng()\nRandom.seed!(rng, 0)","category":"page"},{"location":"examples/generated/beginner/Basics/main/#Arrays","page":"Julia & Lux for the Uninitiated","title":"Arrays","text":"","category":"section"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"The starting point for all of our models is the Array (sometimes referred to as a Tensor in other frameworks). This is really just a list of numbers, which might be arranged into a shape like a square. Let's write down an array with three elements.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"x = [1, 2, 3]","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"Here's a matrix – a square array with four elements.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"x = [1 2; 3 4]","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"We often work with arrays of thousands of elements, and don't usually write them down by hand. Here's how we can create an array of 5×3 = 15 elements, each a random number from zero to one.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"x = rand(rng, 5, 3)","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"There's a few functions like this; try replacing rand with ones, zeros, or randn.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"By default, Julia works stores numbers is a high-precision format called Float64. In ML we often don't need all those digits, and can ask Julia to work with Float32 instead. We can even ask for more digits using BigFloat.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"x = rand(BigFloat, 5, 3)","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"x = rand(Float32, 5, 3)","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"We can ask the array how many elements it has.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"length(x)","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"Or, more specifically, what size it has.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"size(x)","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"We sometimes want to see some elements of the array on their own.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"x","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"x[2, 3]","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"This means get the second row and the third column. We can also get every row of the third column.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"x[:, 3]","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"We can add arrays, and subtract them, which adds or subtracts each element of the array.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"x + x","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"x - x","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"Julia supports a feature called broadcasting, using the . syntax. This tiles small arrays (or single numbers) to fill bigger ones.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"x .+ 1","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"We can see Julia tile the column vector 1:5 across all rows of the larger array.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"zeros(5, 5) .+ (1:5)","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"The x' syntax is used to transpose a column 1:5 into an equivalent row, and Julia will tile that across columns.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"zeros(5, 5) .+ (1:5)'","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"We can use this to make a times table.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"(1:5) .* (1:5)'","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"Finally, and importantly for machine learning, we can conveniently do things like matrix multiply.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"W = randn(5, 10)\nx = rand(10)\nW * x","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"Julia's arrays are very powerful, and you can learn more about what they can do here.","category":"page"},{"location":"examples/generated/beginner/Basics/main/#CUDA-Arrays","page":"Julia & Lux for the Uninitiated","title":"CUDA Arrays","text":"","category":"section"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"CUDA functionality is provided separately by the CUDA.jl package. If you have a GPU and CUDA available, Lux will automatically build the required CUDA dependencies using CUDA.jl.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"You can manually add CUDA. Once CUDA is loaded you can move any array to the GPU with the cu function (or the gpu function exported by Lux`), and it supports all of the above operations with the same syntax.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"# using CUDA\n# x = cu(rand(5, 3))","category":"page"},{"location":"examples/generated/beginner/Basics/main/#(Im)mutability","page":"Julia & Lux for the Uninitiated","title":"(Im)mutability","text":"","category":"section"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"Lux as you might have read is Immutable by convention which means that the core library is built without any form of mutation and all functions are pure. However, we don't enfore it in any form. We do strongly recommend that users extending this framework for their respective applications don't mutate their arrays.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"x = reshape(1:8, 2, 4)","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"To update this array, we should first copy the array.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"x_copy = copy(x)\nview(x_copy, :, 1) .= 0\n\nprintln(\"Original Array \", x)\nprintln(\"Mutated Array \", x_copy)","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"Note that our current default AD engine (Zygote) is unable to differentiate through this mutation, however, for these specialized cases it is quite trivial to write custom backward passes. (This problem will be fixed once we move towards Enzyme.jl)","category":"page"},{"location":"examples/generated/beginner/Basics/main/#Managing-Randomness","page":"Julia & Lux for the Uninitiated","title":"Managing Randomness","text":"","category":"section"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"We relu on the Julia StdLib Random for managing the randomness in our execution. First, we create an PRNG and seed it.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"rng = Random.default_rng() # Creates a Xoshiro PRNG\nRandom.seed!(rng, 0)","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"If we call any function that relies on rng and uses it via randn, rand, etc. rng will be mutated. As we have already established we care a lot about immutability, hence we should use Lux.replicate on PRNG before using them.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"First, let us run a random number generator 3 times with the replicated rng","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"for i in 1:3\n    println(\"Iteration $i \", rand(Lux.replicate(rng), 10))\nend","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"As expected we get the same output. We can remove the replicate call and we will get different outputs","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"for i in 1:3\n    println(\"Iteration $i \", rand(rng, 10))\nend","category":"page"},{"location":"examples/generated/beginner/Basics/main/#Automatic-Differentiation","page":"Julia & Lux for the Uninitiated","title":"Automatic Differentiation","text":"","category":"section"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"Julia has quite a few (maybe too many) AD tools. For the purpose of this tutorial, we will use AbstractDifferentiation.jl which provides an uniform API across multiple AD backends. For the backends we will use:","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"ForwardDiff.jl – For Jacobian-Vector Product (JVP)\nZygote.jl – For Vector-Jacobian Product (VJP)","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"Slight Detour: We have had several questions regarding if we will be considering any other AD system for the reverse-diff backend. For now we will stick to Zygote.jl, however once Enzyme.jl has support for custom rules and we have tested Lux extensively with it, we will make the switch.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"Even though, theoretically, a VJP (Vector-Jacobian product - reverse autodiff) and a JVP (Jacobian-Vector product - forward-mode autodiff) are similar—they compute a product of a Jacobian and a vector—they differ by the computational complexity of the operation. In short, when you have a large number of parameters (hence a wide matrix), a JVP is less efficient computationally than a VJP, and, conversely, a JVP is more efficient when the Jacobian matrix is a tall matrix.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"using ForwardDiff, Zygote, AbstractDifferentiation","category":"page"},{"location":"examples/generated/beginner/Basics/main/#Gradients","page":"Julia & Lux for the Uninitiated","title":"Gradients","text":"","category":"section"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"For our first example, consider a simple function computing f(x) = frac12x^T x, where nabla f(x) = x","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"f(x) = x' * x / 2\n∇f(x) = x\nv = randn(rng, Float32, 4)","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"Let's use AbstractDifferentiation and Zygote to compute the gradients","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"println(\"Actual Gradient: \", ∇f(v))\nprintln(\"Computed Gradient via Reverse Mode AD (Zygote): \",\n        AD.gradient(AD.ZygoteBackend(), f, v)[1])\nprintln(\"Computed Gradient via Forward Mode AD (ForwardDiff): \",\n        AD.gradient(AD.ForwardDiffBackend(), f, v)[1])","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"Note that AD.gradient will only work for scalar valued outputs","category":"page"},{"location":"examples/generated/beginner/Basics/main/#Jacobian-Vector-Product","page":"Julia & Lux for the Uninitiated","title":"Jacobian-Vector Product","text":"","category":"section"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"I will defer the discussion on forward-mode AD to https://book.sciml.ai/notes/08/. Here let us just look at a mini example on how to use it.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"f(x) = x .* x ./ 2\nx = randn(rng, Float32, 5)\nv = ones(Float32, 5)","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"Construct the pushforward function.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"pf_f = AD.value_and_pushforward_function(AD.ForwardDiffBackend(), f, x)","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"Compute the jvp","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"val, jvp = pf_f(v)\nprintln(\"Computed Value: f(\", x, \") = \", val)\nprintln(\"JVP: \", jvp[1])","category":"page"},{"location":"examples/generated/beginner/Basics/main/#Vector-Jacobian-Product","page":"Julia & Lux for the Uninitiated","title":"Vector-Jacobian Product","text":"","category":"section"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"Using the same function and inputs, let us compute the VJP","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"pb_f = AD.value_and_pullback_function(AD.ZygoteBackend(), f, x)","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"Compute the vjp","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"val, vjp = pb_f(v)\nprintln(\"Computed Value: f(\", x, \") = \", val)\nprintln(\"VJP: \", vjp[1])","category":"page"},{"location":"examples/generated/beginner/Basics/main/#Linear-Regression","page":"Julia & Lux for the Uninitiated","title":"Linear Regression","text":"","category":"section"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"Finally, now let us consider a linear regression problem. From a set of data-points left (x_i y_i) i in left 1 dots k right x_i in R^n y_i in R^m right, we try to find a set of parameters W and b, s.t. f_Wb(x) = Wx + b minimizes the mean squared error:","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"L(W b) longrightarrow sum_i = 1^k frac12  y_i - f_Wb(x_i) _2^2","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"We can write f from scratch, but to demonstrate Lux let us use the Dense layer.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"model = Dense(10 => 5)\n\nrng = Random.default_rng()\nRandom.seed!(rng, 0)","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"Let us initialize the parameters and states (in this case it is empty) for the model","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"ps, st = Lux.setup(rng, model)\nps = ps |> ComponentArray","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"Set problem dimensions.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"n_samples = 20\nx_dim = 10\ny_dim = 5","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"Generate random ground truth W and b.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"W = randn(rng, Float32, y_dim, x_dim)\nb = randn(rng, Float32, y_dim)","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"Generate samples with additional noise.","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"x_samples = randn(rng, Float32, x_dim, n_samples)\ny_samples = W * x_samples .+ b .+ 0.01f0 .* randn(rng, Float32, y_dim, n_samples)\nprintln(\"x shape: \", size(x_samples), \"; y shape: \", size(y_samples))","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"For updating our parameters let's use Optimisers.jl","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"using Optimisers\n\nopt = Optimisers.Descent(0.01f0)","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"Initialize the initial state of the optimiser","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"opt_state = Optimisers.setup(opt, ps)","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"Define the loss function","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"mse(model, ps, st, X, y) = sum(abs2, model(X, ps, st)[1] .- y)\nmse(weight, bias, X, y) = sum(abs2, weight * X .+ bias .- y)\nloss_function(ps, X, y) = mse(model, ps, st, X, y)\n\nprintln(\"Loss Value with ground true W & b: \", mse(W, b, x_samples, y_samples))\n\nfor i in 1:100\n    # In actual code, don't use globals. But here I will simply for the sake of demonstration\n    global ps, st, opt_state\n    # Compute the gradient\n    gs = gradient(loss_function, ps, x_samples, y_samples)[1]\n    # Perform parameter update\n    opt_state, ps = Optimisers.update(opt_state, ps, gs)\n    if i % 10 == 1 || i == 100\n        println(\"Loss Value after $i iterations: \",\n                mse(model, ps, st, x_samples, y_samples))\n    end\nend","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"","category":"page"},{"location":"examples/generated/beginner/Basics/main/","page":"Julia & Lux for the Uninitiated","title":"Julia & Lux for the Uninitiated","text":"This page was generated using Literate.jl.","category":"page"},{"location":"api/layers/","page":"Layers","title":"Layers","text":"CurrentModule = Lux","category":"page"},{"location":"api/layers/#Containers","page":"Layers","title":"Containers","text":"","category":"section"},{"location":"api/layers/","page":"Layers","title":"Layers","text":"BranchLayer\nChain\nPairwiseFusion\nParallel\nSkipConnection","category":"page"},{"location":"api/layers/#Lux.BranchLayer","page":"Layers","title":"Lux.BranchLayer","text":"BranchLayer(layers...)\n\nTakes an input x and passes it through all the layers and returns a tuple of the outputs.\n\nArguments\n\nlayers: A list of N Lux layers\n\nInputs\n\nx: Will be directly passed to each of the layers\n\nReturns\n\nTuple: (layer_1(x), layer_2(x), ..., layer_N(x))\nUpdated state of the layers\n\nParameters\n\nParameters of each layer wrapped in a NamedTuple with fields = layer_1, layer_2, ..., layer_N\n\nStates\n\nStates of each layer wrapped in a NamedTuple with fields = layer_1, layer_2, ..., layer_N\n\nComparison with Parallel\n\nThis is slightly different from Parallel(nothing, layers...)\n\nIf the input is a tuple, Parallel will pass each element individually to each layer\nBranchLayer essentially assumes 1 input comes in and is branched out into N outputs\n\nExample\n\nAn easy way to replicate an input to an NTuple is to do\n\nl = BranchLayer(\n    NoOpLayer(),\n    NoOpLayer(),\n    NoOpLayer(),\n)\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Lux.Chain","page":"Layers","title":"Lux.Chain","text":"Chain(layers...; disable_optimizations::Bool = false)\n\nCollects multiple layers / functions to be called in sequence on a given input.\n\nArguments\n\nlayers: A list of N Lux layers\n\nKeyword Arguments\n\ndisable_optimizations: Prevents any structural optimization\n\nInputs\n\nInput x is passed sequentially to each layer, and must conform to the input requirements of the internal layers.\n\nReturns\n\nOutput after sequentially applying all the layers to x\nUpdated model states\n\nParameters\n\nParameters of each layer wrapped in a NamedTuple with fields = layer_1, layer_2, ..., layer_N\n\nStates\n\nStates of each layer wrapped in a NamedTuple with fields = layer_1, layer_2, ..., layer_N\n\nOptimizations\n\nPerforms a few optimizations to generate reasonable architectures. Can be disabled using keyword argument disable_optimizations.\n\nAll sublayers are recursively optimized.\nIf a function f is passed as a layer and it doesn't take 3 inputs, it is converted to a WrappedFunction(f) which takes only one input.\nIf the layer is a Chain, it is flattened.\nNoOpLayers are removed.\nIf there is only 1 layer (left after optimizations), then it is returned without the Chain wrapper.\nIf there are no layers (left after optimizations), a NoOpLayer is returned.\n\nExample\n\nc = Chain(\n    Dense(2, 3, relu),\n    BatchNorm(3),\n    Dense(3, 2)\n)\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Lux.PairwiseFusion","page":"Layers","title":"Lux.PairwiseFusion","text":"PairwiseFusion(connection, layers...)\n\nx1 --> layer1 --> y1\n                  |\n                  |--> connection --> layer2 --> y2\n                  |                              |\n                  x2                             |--> connection --> layer3 --> y3\n                                                 |                              |\n                                                 x3                             |--> connection --> y4\n                                                                                |\n                                                                                x4\n\nArguments\n\nconnection: Takes 2 inputs and combines them\nlayers: AbstractExplicitLayers \n\nInputs\n\nLayer behaves differently based on input type:\n\nInput x is a tuple of length N then the layers must be a tuple of length N. The computation is as follows\n\ny = x[1]\nfor i in 1:N\n    y = connection(x[i], layers[i](y))\nend\n\nAny other kind of input\n\ny = x\nfor i in 1:N\n    y = connection(x, layers[i](y))\nend\n\nReturns\n\nSee Inputs section for how the return value is computed\nUpdated model state for all the contained layers\n\nParameters\n\nParameters of each layer wrapped in a NamedTuple with fields = layer_1, layer_2, ..., layer_N\n\nStates\n\nStates of each layer wrapped in a NamedTuple with fields = layer_1, layer_2, ..., layer_N\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Lux.Parallel","page":"Layers","title":"Lux.Parallel","text":"Parallel(connection, layers...)\n\nCreate a layer which passes an input to each path in layers, before reducing the output with connection.\n\nArguments\n\nlayers: A list of N Lux layers\nconnection: An N-argument function that is called after passing the input through each layer. If connection = nothing, we return a tuple Parallel(nothing, f, g)(x, y) = (f(x), g(y))\n\nInputs\n\nx: if x is not a tuple, then return is computed as connection([l(x) for l in layers]...). Else one is passed to each layer, thus Parallel(+, f, g)(x, y) = f(x) + g(y).\n\nReturns\n\nSee the Inputs section for how the output is computed\nUpdated state of the layers\n\nParameters\n\nParameters of each layer wrapped in a NamedTuple with fields = layer_1, layer_2, ..., layer_N\n\nStates\n\nStates of each layer wrapped in a NamedTuple with fields = layer_1, layer_2, ..., layer_N\n\nSee also SkipConnection which is Parallel with one identity.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Lux.SkipConnection","page":"Layers","title":"Lux.SkipConnection","text":"SkipConnection(layer, connection)\n\nCreate a skip connection which consists of a layer or Chain of consecutive layers and a shortcut connection linking the block's input to the output through a user-supplied 2-argument callable. The first argument to the callable will be propagated through the given layer while the second is the unchanged, \"skipped\" input.\n\nThe simplest \"ResNet\"-type connection is just SkipConnection(layer, +).\n\nArguments\n\nlayer: Layer or Chain of layers to be applied to the input\nconnection: A 2-argument function that takes layer(input) and the input\n\nInputs\n\nx: Will be passed directly to layer\n\nReturns\n\nOutput of connection(layer(input), input)\nUpdated state of layer\n\nParameters\n\nParameters of layer\n\nStates\n\nStates of layer\n\nSee Parallel for a more general implementation.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Convolutional-Layers","page":"Layers","title":"Convolutional Layers","text":"","category":"section"},{"location":"api/layers/","page":"Layers","title":"Layers","text":"Conv","category":"page"},{"location":"api/layers/#Lux.Conv","page":"Layers","title":"Lux.Conv","text":"Conv(k::NTuple{N,Integer}, (in_chs => out_chs)::Pair{<:Integer,<:Integer}, activation=identity; init_weight=glorot_uniform, stride=1, pad=0, dilation=1, groups=1, bias=true)\n\nStandard convolutional layer.\n\nImage data should be stored in WHCN order (width, height, channels, batch). In other words, a 100 × 100 RGB image would be a 100 × 100 × 3 × 1 array, and a batch of 50 would be a 100 × 100 × 3 × 50 array. This has N = 2 spatial dimensions, and needs a kernel size like (5, 5), a 2-tuple of integers. To take convolutions along N feature dimensions, this layer expects as input an array with ndims(x) == N + 2, where size(x, N + 1) == in_chs is the number of input channels, and size(x, ndims(x)) is the number of observations in a batch.\n\nnote: Note\nFrameworks like Pytorch perform cross-correlation in their convolution layers\n\nArguments\n\nk: Tuple of integers specifying the size of the convolutional kernel. Eg, for 2D convolutions length(k) == 2\nin_chs: Number of input channels\nout_chs: Number of input and output channels\nactivation: Activation Function\n\nKeyword Arguments\n\ninit_weight: Controls the initialization of the weight parameter\nstride: Should each be either single integer, or a tuple with N integers\ndilation: Should each be either single integer, or a tuple with N integers\npad: Specifies the number of elements added to the borders of the data array. It can be\na single integer for equal padding all around,\na tuple of N integers, to apply the same padding at begin/end of each spatial dimension,\na tuple of 2*N integers, for asymmetric padding, or\nthe singleton SamePad(), to calculate padding such that size(output,d) == size(x,d) / stride (possibly rounded) for each spatial dimension.\ngroups: Expected to be an Int. It specifies the number of groups to divide a convolution into (set groups = in_chs for Depthwise Convolutions). in_chs and out_chs must be divisible by groups.\nbias: The initial bias vector is all zero by default. Trainable bias can be disabled entirely by setting this to false.\n\nInputs\n\nx: Data satisfying ndims(x) == N + 2 && size(x, N - 1) == in_chs, i.e. size(x) = (I_N, ..., I_1, C_in, N)\n\nReturns\n\nOutput of the convolution y of size (O_N, ..., O_1, C_out, N) where O_i = floorleft(fracI_i + padi + pad(i + N)  length(pad) - dilationi times (ki - 1)stridei + 1right)\nEmpty NamedTuple()\n\nParameters\n\nweight: Convolution kernel\nbias: Bias (present if bias=true)\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Dropout-Layers","page":"Layers","title":"Dropout Layers","text":"","category":"section"},{"location":"api/layers/","page":"Layers","title":"Layers","text":"Dropout\nVariationalHiddenDropout","category":"page"},{"location":"api/layers/#Lux.Dropout","page":"Layers","title":"Lux.Dropout","text":"Dropout(p; dims=:)\n\nDropout layer.\n\nArguments\n\np: Probability of Dropout (if p = 0 then NoOpLayer is returned)\n\nKeyword Arguments\n\nTo apply dropout along certain dimension(s), specify the dims keyword. e.g. Dropout(p; dims = 3) will randomly zero out entire channels on WHCN input (also called 2D dropout).\n\nInputs\n\nx: Must be an AbstractArray\n\nReturns\n\nx with dropout mask applied if training=Val(true) else just x\nState with updated rng\n\nStates\n\nrng: Pseudo Random Number Generator\ntraining: Used to check if training/inference mode\n\nCall Lux.testmode to switch to test mode.\n\nSee also VariationalHiddenDropout\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Lux.VariationalHiddenDropout","page":"Layers","title":"Lux.VariationalHiddenDropout","text":"VariationalHiddenDropout(p; dims=:)\n\nVariationalHiddenDropout layer. The only difference from Dropout is that the mask is retained until Lux.update_state(l, :update_mask, Val(true)) is called.\n\nArguments\n\np: Probability of Dropout (if p = 0 then NoOpLayer is returned)\n\nKeyword Arguments\n\nTo apply dropout along certain dimension(s), specify the dims keyword. e.g. VariationalHiddenDropout(p; dims = 3) will randomly zero out entire channels on WHCN input (also called 2D dropout).\n\nInputs\n\nx: Must be an AbstractArray\n\nReturns\n\nx with dropout mask applied if training=Val(true) else just x\nState with updated rng\n\nStates\n\nrng: Pseudo Random Number Generator\ntraining: Used to check if training/inference mode\nmask: Dropout mask. Initilly set to nothing. After every run, contains the mask applied in that call\nupdate_mask: Stores whether new mask needs to be generated in the current call\n\nCall Lux.testmode to switch to test mode.\n\nSee also Dropout\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Pooling-Layers","page":"Layers","title":"Pooling Layers","text":"","category":"section"},{"location":"api/layers/","page":"Layers","title":"Layers","text":"AdaptiveMaxPool\nAdaptiveMeanPool\nGlobalMaxPool\nGlobalMeanPool\nMaxPool\nMeanPool","category":"page"},{"location":"api/layers/#Lux.AdaptiveMaxPool","page":"Layers","title":"Lux.AdaptiveMaxPool","text":"AdaptiveMaxPool(out::NTuple)\n\nAdaptive Max Pooling layer. Calculates the necessary window size such that its output has size(y)[1:N] == out.\n\nArguments\n\nout: Size of the first N dimensions for the output\n\nInputs\n\nx: Expects as input an array with ndims(x) == N+2, i.e. channel and batch dimensions, after the N feature dimensions, where N = length(out).\n\nReturns\n\nOutput of size (out..., C, N)\nEmpty NamedTuple()\n\nSee also MaxPool, AdaptiveMeanPool.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Lux.AdaptiveMeanPool","page":"Layers","title":"Lux.AdaptiveMeanPool","text":"AdaptiveMeanPool(out::NTuple)\n\nAdaptive Mean Pooling layer. Calculates the necessary window size such that its output has size(y)[1:N] == out.\n\nArguments\n\nout: Size of the first N dimensions for the output\n\nInputs\n\nx: Expects as input an array with ndims(x) == N+2, i.e. channel and batch dimensions, after the N feature dimensions, where N = length(out).\n\nReturns\n\nOutput of size (out..., C, N)\nEmpty NamedTuple()\n\nSee also MeanPool, AdaptiveMaxPool.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Lux.GlobalMaxPool","page":"Layers","title":"Lux.GlobalMaxPool","text":"GlobalMaxPool()\n\nGlobal Mean Pooling layer. Transforms (w,h,c,b)-shaped input into (1,1,c,b)-shaped output, by performing max pooling on the complete (w,h)-shaped feature maps.\n\nInputs\n\nx: Data satisfying ndims(x) > 2, i.e. size(x) = (I_N, ..., I_1, C, N)\n\nReturns\n\nOutput of the pooling y of size (1, ..., 1, C, N)\nEmpty NamedTuple()\n\nSee also MaxPool, AdaptiveMaxPool, GlobalMeanPool\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Lux.GlobalMeanPool","page":"Layers","title":"Lux.GlobalMeanPool","text":"GlobalMeanPool()\n\nGlobal Mean Pooling layer. Transforms (w,h,c,b)-shaped input into (1,1,c,b)-shaped output, by performing mean pooling on the complete (w,h)-shaped feature maps.\n\nInputs\n\nx: Data satisfying ndims(x) > 2, i.e. size(x) = (I_N, ..., I_1, C, N)\n\nReturns\n\nOutput of the pooling y of size (1, ..., 1, C, N)\nEmpty NamedTuple()\n\nSee also MeanPool, AdaptiveMeanPool, GlobalMaxPool\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Lux.MaxPool","page":"Layers","title":"Lux.MaxPool","text":"MaxPool(window::NTuple; pad=0, stride=window)\n\nMax pooling layer, which replaces all pixels in a block of size window with the maximum value.\n\nArguments\n\nwindow: Tuple of integers specifying the size of the window. Eg, for 2D pooling length(window) == 2\n\nKeyword Arguments\n\nstride: Should each be either single integer, or a tuple with N integers\npad: Specifies the number of elements added to the borders of the data array. It can be\na single integer for equal padding all around,\na tuple of N integers, to apply the same padding at begin/end of each spatial dimension,\na tuple of 2*N integers, for asymmetric padding, or\nthe singleton SamePad(), to calculate padding such that size(output,d) == size(x,d) / stride (possibly rounded) for each spatial dimension.\n\nInputs\n\nx: Data satisfying ndims(x) == N + 2, i.e. size(x) = (I_N, ..., I_1, C, N)\n\nReturns\n\nOutput of the pooling y of size (O_N, ..., O_1, C, N) where O_i = floorleft(fracI_i + padi + pad(i + N)  length(pad) - dilationi times (ki - 1)stridei + 1right)\nEmpty NamedTuple()\n\nSee also Conv, MeanPool, GlobalMaxPool, AdaptiveMaxPool\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Lux.MeanPool","page":"Layers","title":"Lux.MeanPool","text":"MeanPool(window::NTuple; pad=0, stride=window)\n\nMean pooling layer, which replaces all pixels in a block of size window with the mean value.\n\nArguments\n\nwindow: Tuple of integers specifying the size of the window. Eg, for 2D pooling length(window) == 2\n\nKeyword Arguments\n\nstride: Should each be either single integer, or a tuple with N integers\npad: Specifies the number of elements added to the borders of the data array. It can be\na single integer for equal padding all around,\na tuple of N integers, to apply the same padding at begin/end of each spatial dimension,\na tuple of 2*N integers, for asymmetric padding, or\nthe singleton SamePad(), to calculate padding such that size(output,d) == size(x,d) / stride (possibly rounded) for each spatial dimension.\n\nInputs\n\nx: Data satisfying ndims(x) == N + 2, i.e. size(x) = (I_N, ..., I_1, C, N)\n\nReturns\n\nOutput of the pooling y of size (O_N, ..., O_1, C, N) where O_i = floorleft(fracI_i + padi + pad(i + N)  length(pad) - dilationi times (ki - 1)stridei + 1right)\nEmpty NamedTuple()\n\nSee also Conv, MaxPool, GlobalMeanPool, AdaptiveMeanPool\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Recurrent-Layers","page":"Layers","title":"Recurrent Layers","text":"","category":"section"},{"location":"api/layers/","page":"Layers","title":"Layers","text":"warning: Warning\nRecurrent Layers API should be considered Experimental at this point","category":"page"},{"location":"api/layers/","page":"Layers","title":"Layers","text":"GRUCell\nLSTMCell\nRNNCell","category":"page"},{"location":"api/layers/#Lux.GRUCell","page":"Layers","title":"Lux.GRUCell","text":"GRUCell((in_dims, out_dims)::Pair{<:Int,<:Int}; init_weight::Tuple{Function,Function,Function}=(glorot_uniform, glorot_uniform, glorot_uniform), init_bias::Tuple{Function,Function,Function}=(zeros32, zeros32, zeros32), init_state::Function=zeros32)\n\nGated Recurrent Unit (GRU) Cell\n\nr = sigma(W_ir times x + W_hr times h_prev + b_hr)\n\nz = sigma(W_iz times x + W_hz times h_prev + b_hz)\n\nn = sigma(W_in times x + b_in + r cdot (W_hn times h_prev + b_hn))\n\nh_new = (1 - z) cdot n + z cdot h_prev\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State) Dimension\ninit_bias: Initializer for bias. Must be a tuple containing 3 functions\ninit_weight: Initializer for weight. Must be a tuple containing 3 functions\ninit_state: Initializer for hidden state\n\nInputs\n\nCase 1: Only a single input x of shape (in_dims, batch_size) - Creates a hidden state using init_state and proceeds to Case 2.\nCase 2: Tuple (x, h) is provided, then the updated hidden state is returned.\n\nReturns\n\nNew hidden state h_new of shape (out_dims, batch_size)\nUpdated model state\n\nParameters\n\nweight_i: Concatenated Weights to map from input space left W_ir W_iz W_in right.\nweight_h: Concatenated Weights to map from hidden space left W_hr W_hz W_hn right\nbias_i: Bias vector (b_in)\nbias_h: Concatenated Bias vector for the hidden space left b_hr b_hz b_hn right\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Lux.LSTMCell","page":"Layers","title":"Lux.LSTMCell","text":"LSTMCell(in_dims => out_dims; init_weight=(glorot_uniform, glorot_uniform, glorot_uniform, glorot_uniform), init_bias=(zeros32, zeros32, ones32, zeros32), init_state=zeros32)\n\nLong Short-Term (LSTM) Cell\n\ni = sigma(W_ii times x + W_hi times h_prev + b_i)\n\nf = sigma(W_if times x + W_hf times h_prev + b_f)\n\ng = tanh(W_ig times x + W_hg times h_prev + b_g)\n\no = sigma(W_io times x + W_ho times h_prev + b_o)\n\nc_new = f cdot c_prev + i cdot g\n\nh_new = o cdot tanh(c_new)\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State & Memory) Dimension\ninit_bias: Initializer for bias. Must be a tuple containing 4 functions\ninit_weight: Initializer for weight. Must be a tuple containing 4 functions\ninit_state: Initializer for hidden state and memory\n\nInputs\n\nCase 1: Only a single input x of shape (in_dims, batch_size) - Creates a hidden state and memory using init_state and proceeds to Case 2.\nCase 2: Tuple (x, h, c) is provided, then the updated hidden state and memory is returned.\n\nReturns\n\nTuple Containing\nNew hidden state h_new of shape (out_dims, batch_size)\nUpdated Memory c_new of shape (out_dims, batch_size)\nUpdated model state\n\nParameters\n\nweight_i: Concatenated Weights to map from input space left W_ii W_if W_ig W_io right.\nweight_h: Concatenated Weights to map from hidden space left W_hi W_hf W_hg W_ho right\nbias: Bias vector\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Lux.RNNCell","page":"Layers","title":"Lux.RNNCell","text":"RNNCell(in_dims => out_dims, activation=tanh; bias::Bool=true, init_bias=zeros32, init_weight=glorot_uniform, init_state=ones32)\n\nAn Elman RNNCell cell with activation (typically set to tanh or relu).\n\nh_new = activation(weight_ih times x + weight_hh times h_prev + bias)\n\nArguments\n\nin_dims: Input Dimension\nout_dims: Output (Hidden State) Dimension\nactivation: Activation function\nbias: Set to false to deactivate bias\ninit_bias: Initializer for bias\ninit_weight: Initializer for weight\ninit_state: Initializer for hidden state\n\nInputs\n\nCase 1: Only a single input x of shape (in_dims, batch_size) - Creates a hidden state using init_state and proceeds to Case 2.\nCase 2: Tuple (x, h) is provided, then the updated hidden state is returned.\n\nReturns\n\nNew hidden state h_new of shape (out_dims, batch_size)\nUpdated model state\n\nParameters\n\nweight_ih: Maps the input to the hidden state.\nweight_hh: Maps the hidden state to the hidden state.\nbias: Bias vector (not present if bias=false)\n\nStates\n\nrng: Controls the randomness (if any) in the initial state generation\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Linear-Layers","page":"Layers","title":"Linear Layers","text":"","category":"section"},{"location":"api/layers/","page":"Layers","title":"Layers","text":"Dense\nScale","category":"page"},{"location":"api/layers/#Lux.Dense","page":"Layers","title":"Lux.Dense","text":"Dense(in_dims => out_dims, activation=identity; init_weight=glorot_uniform, init_bias=zeros32, bias::Bool=true)\n\nCreate a traditional fully connected layer, whose forward pass is given by: y = activation.(weight * x .+ bias)\n\nArguments\n\nin_dims: number of input dimensions\nout_dims: number of output dimensions\nactivation: activation function\n\nKeyword Arguments\n\ninit_weight: initializer for the weight matrix (weight = init_weight(rng, out_dims, in_dims))\ninit_bias: initializer for the bias vector (ignored if bias=false)\nbias: whether to include a bias vector\n\nInput\n\nx must be a Matrix of size in_dims × B or a Vector of length in_dims\n\nReturns\n\nMatrix of size out_dims × B or a Vector of length out_dims\nEmpty NamedTuple()\n\nParameters\n\nweight: Weight Matrix of size out_dims × in_dims\nbias: Bias of size out_dims × 1 (present if bias=true)\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Lux.Scale","page":"Layers","title":"Lux.Scale","text":"Scale(dims, activation=identity; init_weight=ones32, init_bias=zeros32, bias::Bool=true)\n\nCreate a Sparsely Connected Layer with a very specific structure (only Diagonal Elements are non-zero). The forward pass is given by: y = activation.(weight .* x .+ bias)\n\nArguments\n\ndims: number of input and output dimensions\nactivation: activation function\n\nKeyword Arguments\n\ninit_weight: initializer for the weight matrix (weight = init_weight(rng, out_dims, in_dims))\ninit_bias: initializer for the bias vector (ignored if bias=false)\nbias: whether to include a bias vector\n\nInput\n\nx must be a Matrix of size dims × B or a Vector of length dims\n\nReturns\n\nMatrix of size dims × B or a Vector of length dims\nEmpty NamedTuple()\n\nParameters\n\nweight: Weight Vector of size (dims,)\nbias: Bias of size (dims,)\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Misc.-Helper-Layers","page":"Layers","title":"Misc. Helper Layers","text":"","category":"section"},{"location":"api/layers/","page":"Layers","title":"Layers","text":"ActivationFunction\nFlattenLayer\nNoOpLayer\nReshapeLayer\nSelectDim\nWrappedFunction","category":"page"},{"location":"api/layers/#Lux.ActivationFunction","page":"Layers","title":"Lux.ActivationFunction","text":"ActivationFunction(f)\n\nBroadcast f on the input but fallback to CUDNN for Backward Pass. Internally calls Lux.applyactivation\n\nArguments\n\nf: Activation function\n\nInputs\n\nx: Any array type s.t. f can be broadcasted over it\n\nReturns\n\nBroadcasted Activation f.(x)\nEmpty NamedTuple()\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Lux.FlattenLayer","page":"Layers","title":"Lux.FlattenLayer","text":"FlattenLayer()\n\nFlattens the passed array into a matrix.\n\nInputs\n\nx: AbstractArray\n\nReturns\n\nAbstractMatrix of size (:, size(x, ndims(x)))\nEmpty NamedTuple()\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Lux.NoOpLayer","page":"Layers","title":"Lux.NoOpLayer","text":"NoOpLayer()\n\nAs the name suggests does nothing but allows pretty printing of layers. Whatever input is passed is returned.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Lux.ReshapeLayer","page":"Layers","title":"Lux.ReshapeLayer","text":"ReshapeLayer(dims)\n\nReshapes the passed array to have a size of (dims..., :)\n\nArguments\n\ndims: The new dimensions of the array (excluding the last dimension).\n\nInputs\n\nx: AbstractArray of any shape which can be reshaped in (dims..., size(x, ndims(x)))\n\nReturns\n\nAbstractArray of size (dims..., size(x, ndims(x)))\nEmpty NamedTuple()\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Lux.SelectDim","page":"Layers","title":"Lux.SelectDim","text":"SelectDim(dim, i)\n\nReturn a view of all the data of the input x where the index for dimension dim equals i. Equivalent to view(x,:,:,...,i,:,:,...) where i is in position d.\n\nArguments\n\ndim: Dimension for indexing\ni: Index for dimension dim\n\nInputs\n\nx: AbstractArray that can be indexed with view(x,:,:,...,i,:,:,...)\n\nReturns\n\nview(x,:,:,...,i,:,:,...) where i is in position d\nEmpty NamedTuple()\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Lux.WrappedFunction","page":"Layers","title":"Lux.WrappedFunction","text":"WrappedFunction(f)\n\nWraps a stateless and parameter less function. Might be used when a function is added to Chain. For example, Chain(x -> relu.(x)) would not work and the right thing to do would be Chain((x, ps, st) -> (relu.(x), st)). An easier thing to do would be Chain(WrappedFunction(Base.Fix1(broadcast, relu)))\n\nArguments\n\nf::Function: A stateless and parameterless function\n\nInputs\n\nx: s.t hasmethod(f, (typeof(x),)) is true\n\nReturns\n\nOutput of f(x)\nEmpty NamedTuple()\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Normalization-Layers","page":"Layers","title":"Normalization Layers","text":"","category":"section"},{"location":"api/layers/","page":"Layers","title":"Layers","text":"BatchNorm\nGroupNorm\nWeightNorm","category":"page"},{"location":"api/layers/#Lux.BatchNorm","page":"Layers","title":"Lux.BatchNorm","text":"BatchNorm(chs::Integer, activation=identity; init_bias=zeros32, init_scale=ones32, affine=true, track_stats=true, epsilon=1f-5, momentum=0.1f0)\n\nBatch Normalization layer.\n\nBatchNorm computes the mean and variance for each D_1×...×D_{N-2}×1×D_N input slice and normalises the input accordingly.\n\nArguments\n\nchs: Size of the channel dimension in your data. Given an array with N dimensions, call the N-1th the channel dimension. For a batch of feature vectors this is just the data dimension, for WHCN images it's the usual channel dimension.\nactivation: After normalisation, elementwise activation activation is applied.\n\nKeyword Arguments\n\nIf affine=true, it also applies  a shift and a rescale to the input through to learnable per-channel bias bias and scale scale parameters.\ninit_bias: Controls how the bias is initiliazed\ninit_scale: Controls how the scale is initiliazed\nIf track_stats=true, accumulates mean and variance statistics in training phase that will be used to renormalize the input in test phase.\nepsilon: a value added to the denominator for numerical stability\nmomentum:  the value used for the running_mean and running_var computation\n\nInputs\n\nx: Array where size(x, N - 1) = chs and ndims(x) > 2\n\nReturns\n\ny: Normalized Array\nUpdate model state\n\nParameters\n\naffine=true\nbias: Bias of shape (chs,) \nscale: Scale of shape (chs,)\naffine=false - Empty NamedTuple()\n\nStates\n\nStatistics if track_stats=true\nrunning_mean: Running mean of shape (chs,)\nrunning_var: Running variance of shape (chs,)\nStatistics if track_stats=false\nrunning_mean: nothing\nrunning_var: nothing\ntraining: Used to check if training/inference mode\n\nUse Lux.testmode during inference.\n\nExample\n\nm = Chain(\n    Dense(784 => 64),\n    BatchNorm(64, relu),\n    Dense(64 => 10),\n    BatchNorm(10)\n)\n\nSee also GroupNorm\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Lux.GroupNorm","page":"Layers","title":"Lux.GroupNorm","text":"GroupNorm(chs::Integer, groups::Integer, activation=identity; init_bias=zeros32, init_scale=ones32, affine=true, track_stats=false, epsilon=1f-5, momentum=0.1f0)\n\nGroup Normalization layer.\n\nArguments\n\nchs: Size of the channel dimension in your data. Given an array with N dimensions, call the N-1th the channel dimension. For a batch of feature vectors this is just the data dimension, for WHCN images it's the usual channel dimension.\ngroups is the number of groups along which the statistics are computed. The number of channels must be an integer multiple of the number of groups.\nactivation: After normalisation, elementwise activation activation is applied.\n\nKeyword Arguments\n\nIf affine=true, it also applies  a shift and a rescale to the input through to learnable per-channel bias bias and scale scale parameters.\ninit_bias: Controls how the bias is initiliazed\ninit_scale: Controls how the scale is initiliazed\nIf track_stats=true, accumulates mean and variance statistics in training phase that will be used to renormalize the input in test phase.\nepsilon: a value added to the denominator for numerical stability\nmomentum:  the value used for the running_mean and running_var computation\n\nInputs\n\nx: Array where size(x, N - 1) = chs and ndims(x) > 2\n\nReturns\n\ny: Normalized Array\nUpdate model state\n\nParameters\n\naffine=true\nbias: Bias of shape (chs,) \nscale: Scale of shape (chs,)\naffine=false - Empty NamedTuple()\n\nStates\n\nStatistics if track_stats=true\nrunning_mean: Running mean of shape (groups,)\nrunning_var: Running variance of shape (groups,)\nStatistics if track_stats=false\nrunning_mean: nothing\nrunning_var: nothing\ntraining: Used to check if training/inference mode\n\nUse Lux.testmode during inference.\n\nExample\n\nm = Chain(\n    Dense(784 => 64),\n    GroupNorm(64, 4, relu),\n    Dense(64 => 10),\n    GroupNorm(10, 5)\n)\n\nwarning: Warning\nGroupNorm doesn't have CUDNN support. The GPU fallback is not very efficient.\n\nSee also BatchNorm\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Lux.WeightNorm","page":"Layers","title":"Lux.WeightNorm","text":"WeightNorm(layer::AbstractExplicitLayer, which_params::NTuple{N,Symbol}, dims::Union{Tuple,Nothing}=nothing)\n\nApplies weight normalization to a parameter in the given layer.\n\nw = gfracvv\n\nWeight normalization is a reparameterization that decouples the magnitude of a weight tensor from its direction. This updates the parameters in which_params (e.g. weight) using two parameters: one specifying the magnitude (e.g. weight_g) and one specifying the direction (e.g. weight_v).\n\nArguments\n\nlayer whose parameters are being reparameterized\nwhich_params: parameter names for the parameters being reparameterized\nBy default, a norm over the entire array is computed. Pass dims to modify the dimension.\n\nInputs\n\nx: Should be of valid type for input to layer\n\nReturns\n\nOutput from layer\nUpdated model state of layer\n\nParameters\n\nnormalized: Parameters of layer that are being normalized\nunnormalized: Parameters of layer that are not being normalized\n\nStates\n\nSame as that of layer\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Upsampling","page":"Layers","title":"Upsampling","text":"","category":"section"},{"location":"api/layers/","page":"Layers","title":"Layers","text":"Upsample","category":"page"},{"location":"api/layers/#Lux.Upsample","page":"Layers","title":"Lux.Upsample","text":"Upsample(mode = :nearest; [scale, size]) \nUpsample(scale, mode = :nearest)\n\nUpsampling Layer.\n\nLayer Construction\n\nOption 1\n\nmode: Set to :nearest, :linear, :bilinear or :trilinear\n\nOne of two keywords must be given:\n\nIf scale is a number, this applies to all but the last two dimensions (channel and batch) of the input.  It may also be a tuple, to control dimensions individually.\nAlternatively, keyword size accepts a tuple, to directly specify the leading dimensions of the output.\n\nOption 2\n\nIf scale is a number, this applies to all but the last two dimensions (channel and batch) of the input.  It may also be a tuple, to control dimensions individually.\nmode: Set to :nearest, :linear, :bilinear or :trilinear\n\nCurrently supported upsampling modes and corresponding NNlib's methods are:\n\n:nearest -> NNlib.upsample_nearest\n:linear -> NNlib.upsample_linear\n:bilinear -> NNlib.upsample_bilinear\n:trilinear -> NNlib.upsample_trilinear\n\nInputs\n\nx: For the input dimensions look into the documentation for the corresponding NNlib function\nAs a rule of thumb, :nearest should work with arrays of arbitrary dimensions\n:linear works with 3D Arrays, :bilinear works with 4D Arrays, and :trilinear works with 5D Arrays\n\nReturns\n\nUpsampled Input of size size or of size (I_1 × scale[1], ..., I_N × scale[N], C, N)\nEmpty NamedTuple()\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Index","page":"Layers","title":"Index","text":"","category":"section"},{"location":"api/layers/","page":"Layers","title":"Layers","text":"Pages = [\"layers.md\"]","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"EditURL = \"https://github.com/avik-pal/Lux.jl/blob/main/examples/BayesianNN/main.jl\"","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/#Bayesian-Neural-Network","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"","category":"section"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"We borrow this tutorial from the official Turing Docs. We will show how the explicit parameterization of Lux enables first-class composability with packages which expect flattened out parameter vectors.","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"We will use Turing.jl with Lux.jl to implement implementing a classification algorithm. Lets start by importing the relevant libraries","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"# Import libraries.\nusing Lux\nusing Pkg #hide\nPkg.activate(joinpath(dirname(pathof(Lux)), \"..\", \"examples\")) #hide\nusing Turing, Plots, Random, ReverseDiff, NNlib, Functors\n\n# Hide sampling progress.\nTuring.setprogress!(false);\n\n# Use reverse_diff due to the number of parameters in neural networks.\nTuring.setadbackend(:reversediff)","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/#Generating-data","page":"Bayesian Neural Network","title":"Generating data","text":"","category":"section"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"Our goal here is to use a Bayesian neural network to classify points in an artificial dataset. The code below generates data points arranged in a box-like pattern and displays a graph of the dataset we'll be working with.","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"# Number of points to generate.\nN = 80\nM = round(Int, N / 4)\nrng = Random.default_rng()\nRandom.seed!(rng, 1234)\n\n# Generate artificial data.\nx1s = rand(rng, Float32, M) * 4.5f0;\nx2s = rand(rng, Float32, M) * 4.5f0;\nxt1s = Array([[x1s[i] + 0.5f0; x2s[i] + 0.5f0] for i in 1:M])\nx1s = rand(rng, Float32, M) * 4.5f0;\nx2s = rand(rng, Float32, M) * 4.5f0;\nappend!(xt1s, Array([[x1s[i] - 5.0f0; x2s[i] - 5.0f0] for i in 1:M]))\n\nx1s = rand(rng, Float32, M) * 4.5f0;\nx2s = rand(rng, Float32, M) * 4.5f0;\nxt0s = Array([[x1s[i] + 0.5f0; x2s[i] - 5.0f0] for i in 1:M])\nx1s = rand(rng, Float32, M) * 4.5f0;\nx2s = rand(rng, Float32, M) * 4.5f0;\nappend!(xt0s, Array([[x1s[i] - 5.0f0; x2s[i] + 0.5f0] for i in 1:M]))\n\n# Store all the data for later.\nxs = [xt1s; xt0s]\nts = [ones(2 * M); zeros(2 * M)]\n\n# Plot data points.\nfunction plot_data()\n    x1 = first.(xt1s)\n    y1 = last.(xt1s)\n    x2 = first.(xt0s)\n    y2 = last.(xt0s)\n\n    plt = Plots.scatter(x1, y1; color=\"red\", clim=(0, 1))\n    Plots.scatter!(plt, x2, y2; color=\"blue\", clim=(0, 1))\n\n    return plt\nend\n\nplot_data()","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/#Building-the-Neural-Network","page":"Bayesian Neural Network","title":"Building the Neural Network","text":"","category":"section"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"The next step is to define a feedforward neural network where we express our parameters as distributions, and not single points as with traditional neural networks. For this we will use Dense to define liner layers and compose them via Chain, both are neural network primitives from Lux. The network nn we will creat will have two hidden layers with tanh activations and one output layer with sigmoid activation, as shown below.","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"The nn is an instance that acts as a function and can take data, parameters and current state as inputs and output predictions. We will define distributions on the neural network parameters.","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"# Construct a neural network using Lux\nnn = Chain(Dense(2, 3, tanh), Dense(3, 2, tanh), Dense(2, 1, sigmoid))\n\n# Initialize the model weights and state\nps, st = Lux.setup(rng, nn)\n\nLux.parameterlength(nn) # number of paraemters in NN","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"The probabilistic model specification below creates a parameters variable, which has IID normal variables. The parameters represents all parameters of our neural net (weights and biases).","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"# Create a regularization term and a Gaussian prior variance term.\nalpha = 0.09\nsig = sqrt(1.0 / alpha)","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"Construct named tuple from a sampled parameter vector. We could also use ComponentArrays here and simply broadcast to avoid doing this. But let's do it this way to avoid dependencies.","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"function vector_to_parameters(ps_new::AbstractVector, ps::NamedTuple)\n    @assert length(ps_new) == Lux.parameterlength(ps)\n    i = 1\n    function get_ps(x)\n        z = reshape(view(ps_new, i:(i + length(x) - 1)), size(x))\n        i += length(x)\n        return z\n    end\n    return fmap(get_ps, ps)\nend\n\n# Specify the probabilistic model.\n@model function bayes_nn(xs, ts)\n    global st\n\n    # Sample the parameters\n    nparameters = Lux.parameterlength(nn)\n    parameters ~ MvNormal(zeros(nparameters), sig .* ones(nparameters))\n\n    # Forward NN to make predictions\n    preds, st = nn(xs, vector_to_parameters(parameters, ps), st)\n\n    # Observe each prediction.\n    for i in 1:length(ts)\n        ts[i] ~ Bernoulli(preds[i])\n    end\nend","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"Inference can now be performed by calling sample. We use the HMC sampler here.","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"# Perform inference.\nN = 5000\nch = sample(bayes_nn(hcat(xs...), ts), HMC(0.05, 4), N)","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"Now we extract the parameter samples from the sampled chain as theta (this is of size 5000 x 20 where 5000 is the number of iterations and 20 is the number of parameters). We'll use these primarily to determine how good our model's classifier is.","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"# Extract all weight and bias parameters.\ntheta = MCMCChains.group(ch, :parameters).value;\nnothing #hide","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/#Prediction-Visualization","page":"Bayesian Neural Network","title":"Prediction Visualization","text":"","category":"section"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"# A helper to run the nn through data `x` using parameters `theta`\nnn_forward(x, theta) = nn(x, vector_to_parameters(theta, ps), st)[1]\n\n# Plot the data we have.\nplot_data()\n\n# Find the index that provided the highest log posterior in the chain.\n_, i = findmax(ch[:lp])\n\n# Extract the max row value from i.\ni = i.I[1]\n\n# Plot the posterior distribution with a contour plot\nx1_range = collect(range(-6; stop=6, length=25))\nx2_range = collect(range(-6; stop=6, length=25))\nZ = [nn_forward([x1, x2], theta[i, :])[1] for x1 in x1_range, x2 in x2_range]\ncontour!(x1_range, x2_range, Z)","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"The contour plot above shows that the MAP method is not too bad at classifying our data. Now we can visualize our predictions.","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"p(tildex  X alpha) = int_theta p(tildex  theta) p(theta  X alpha) approx sum_theta sim p(theta  X alpha)f_theta(tildex)","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"The nn_predict function takes the average predicted value from a network parameterized by weights drawn from the MCMC chain.","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"# Return the average predicted value across multiple weights.\nfunction nn_predict(x, theta, num)\n    return mean([nn_forward(x, view(theta, i, :))[1] for i in 1:10:num])\nend","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"Next, we use the nn_predict function to predict the value at a sample of points where the x1 and x2 coordinates range between -6 and 6. As we can see below, we still have a satisfactory fit to our data, and more importantly, we can also see where the neural network is uncertain about its predictions much easier–-those regions between cluster boundaries.","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"Plot the average prediction.","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"plot_data()\n\nn_end = 1500\nx1_range = collect(range(-6; stop=6, length=25))\nx2_range = collect(range(-6; stop=6, length=25))\nZ = [nn_predict([x1, x2], theta, n_end)[1] for x1 in x1_range, x2 in x2_range]\ncontour!(x1_range, x2_range, Z)","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"Suppose we are interested in how the predictive power of our Bayesian neural network evolved between samples. In that case, the following graph displays an animation of the contour plot generated from the network weights in samples 1 to 1,000.","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"# Number of iterations to plot.\nn_end = 1000\n\nanim = @gif for i in 1:n_end\n    plot_data()\n    Z = [nn_forward([x1, x2], theta[i, :])[1] for x1 in x1_range, x2 in x2_range]\n    contour!(x1_range, x2_range, Z; title=\"Iteration $i\", clim=(0, 1))\nend every 5","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"","category":"page"},{"location":"examples/generated/intermediate/BayesianNN/main/","page":"Bayesian Neural Network","title":"Bayesian Neural Network","text":"This page was generated using Literate.jl.","category":"page"},{"location":"design/documentation/#Guide-to-Documentation-for-Lux.jl","page":"Documentation","title":"Guide to Documentation for Lux.jl","text":"","category":"section"},{"location":"design/documentation/#Documentation-for-Layers","page":"Documentation","title":"Documentation for Layers","text":"","category":"section"},{"location":"design/documentation/","page":"Documentation","title":"Documentation","text":"The first line must be indented by 4 spaces and should contain the possible ways to construct the layer. This should be followed up with a description about what the layer does. If mathematical equations are needed to explain what the layer does, go for it. Often times we fuse parameters to make computation faster, this should be reflected in the equations being used, i.e. equations and the internal code must be consistent. (See LSTMCell, GRUCell for some examples)","category":"page"},{"location":"design/documentation/","page":"Documentation","title":"Documentation","text":"note: Note\nThere is no need to document how the layers are being called since they must adhere to layer(x, ps, st). Any deviation from that and the PR will not be accepted.","category":"page"},{"location":"design/documentation/","page":"Documentation","title":"Documentation","text":"Next, we will have certain subsections (though all of them might not be necessary for all layers)","category":"page"},{"location":"design/documentation/","page":"Documentation","title":"Documentation","text":"Arguments: This section should be present unless the layer is constructed without any arguments (See NoOpLayer). All the arguments and their explicit constraints must be explained.\nIt is recommended to separate out the Keyword Arguments in their own section\nInputs: This section should always be present. List out the requirements x needs to satisfy. (don't write about ps and st since that is expected by default)\nReturns: What will the layer return? We know the second element will be a state but is that updated in any form or not? \nParameters: What are the properties of the NamedTuple returned from initialparameters? Omit if the layer is parameterless\nStates: What are the properties of the NamedTuple returned from initialstates? Omit if the layer is stateless","category":"page"},{"location":"introduction/overview/#Why-we-wrote-Lux?","page":"All about Lux","title":"Why we wrote Lux?","text":"","category":"section"},{"location":"introduction/overview/","page":"All about Lux","title":"All about Lux","text":"Julia already has quite a few well established Neural Network Frameworks – Flux & KNet. However, certain design elements – Coupled Model and Parameters & Internal Mutations – associated with these frameworks make them less compiler and user friendly. Making changes to address these problems in the respective frameworks would be too disruptive for users. Here comes in Lux a neural network framework built completely using pure functions to make it both compiler and autodiff friendly.","category":"page"},{"location":"introduction/overview/#Design-Principles","page":"All about Lux","title":"Design Principles","text":"","category":"section"},{"location":"introduction/overview/","page":"All about Lux","title":"All about Lux","text":"Layers must be immutable – cannot store any parameter/state but rather store the information to construct them\nLayers are pure functions\nLayers return a Tuple containing the result and the updated state\nGiven same inputs the outputs must be same – yes this must hold true even for stochastic functions. Randomness must be controlled using rngs passed in the state.\nEasily extendible","category":"page"},{"location":"introduction/overview/#AbstractExplicitLayer-API","page":"All about Lux","title":"AbstractExplicitLayer API","text":"","category":"section"},{"location":"introduction/overview/","page":"All about Lux","title":"All about Lux","text":"We provide 2 abstract layers:","category":"page"},{"location":"introduction/overview/","page":"All about Lux","title":"All about Lux","text":"AbstractExplicitLayer: Useful for Base Layers and needs to define the following functions\ninitialparameters(rng::AbstractRNG, layer::CustomAbstractExplicitLayer) – This returns a ComponentArray/NamedTuple containing the trainable parameters for the layer.\ninitialstates(rng::AbstractRNG, layer::CustomAbstractExplicitLayer) – This returns a NamedTuple containing the current state for the layer. For most layers this is typically empty. Layers that would potentially contain this include BatchNorm, LSTM, GRU etc.\nparameterlength(layer::CustomAbstractExplicitLayer) – These can be automatically calculated, but it is recommended that the user defines these.\nstatelength(layer::CustomAbstractExplicitLayer) – These can be automatically calculated, but it is recommended that the user defines these.\nAbstractExplicitContainerLayer: Used when the layer is storing other AbstractExplicitLayers or AbstractExplicitContainerLayers. This allows good defaults of the dispatches for functions mentioned in the previous point.","category":"page"},{"location":"introduction/overview/","page":"All about Lux","title":"All about Lux","text":"note: Note\nWe recommend users to subtype their custom layers using AbstractExplicitLayer or AbstractExplicitContainerLayer. However, this is not mandatory.","category":"page"},{"location":"introduction/overview/#Why-use-Lux-over-Flux?","page":"All about Lux","title":"Why use Lux over Flux?","text":"","category":"section"},{"location":"introduction/overview/","page":"All about Lux","title":"All about Lux","text":"Large Neural Networks\nFor small neural networks we recommend SimpleChains.jl.\nFor SciML Applications (Neural ODEs, Deep Equilibrium Models) solvers typically expect a monolithic parameter vector. Flux enables this via its destructure mechanism, however, it often leads to weird bugs. Lux forces users to make an explicit distinction between state variables and parameter variables to avoid these issues.\nComes battery-included for distributed training using FluxMPI.jl\nSensible display of Custom Layers – Ever wanted to see Pytorch like Network printouts or wondered how to extend the pretty printing of Flux's layers. Lux handles all of that by default.\nLess Bug-ridden Code\nNo arbitrary internal mutations – all layers are implemented as pure functions.\nAll layers are deterministic given the parameter and state – if the layer is supposed to be stochastic (say Dropout), the state must contain a seed which is then updated after the function call.\nEasy Parameter Manipulation – Wondering why Flux doesn't have WeightNorm, SpectralNorm, etc. The implicit parameter handling makes it extremely hard to pass parameters around without mutations which AD systems don't like. With Lux implementing them is outright simple.","category":"page"},{"location":"examples/","page":"Additional Examples","title":"Additional Examples","text":"warning: Warning\nThese were not written in the form of tutorials but standalone scripts/packages for people to use","category":"page"},{"location":"examples/#Packages","page":"Additional Examples","title":"Packages","text":"","category":"section"},{"location":"examples/","page":"Additional Examples","title":"Additional Examples","text":"Deep Equilibrium Models","category":"page"},{"location":"examples/#Scipts","page":"Additional Examples","title":"Scipts","text":"","category":"section"},{"location":"examples/","page":"Additional Examples","title":"Additional Examples","text":"ImageNet Classification using Metalhead.jl Models","category":"page"},{"location":"#Lux","page":"Lux: Explicitly Parameterized Neural Networks","title":"Lux","text":"","category":"section"},{"location":"","page":"Lux: Explicitly Parameterized Neural Networks","title":"Lux: Explicitly Parameterized Neural Networks","text":"Lux is a julia deep learning framework which decouples models and parameterization using deeply nested named tuples.","category":"page"},{"location":"","page":"Lux: Explicitly Parameterized Neural Networks","title":"Lux: Explicitly Parameterized Neural Networks","text":"Functional Layer API – Pure Functions and Deterministic Function Calls.\nNo more implicit parameterization – Zygote.Params. Everything is a NamedTuple.\nCompiler and AD-friendly Neural Networks","category":"page"},{"location":"#Installation","page":"Lux: Explicitly Parameterized Neural Networks","title":"Installation","text":"","category":"section"},{"location":"","page":"Lux: Explicitly Parameterized Neural Networks","title":"Lux: Explicitly Parameterized Neural Networks","text":"Install julia v1.6 or above.","category":"page"},{"location":"","page":"Lux: Explicitly Parameterized Neural Networks","title":"Lux: Explicitly Parameterized Neural Networks","text":"using Pkg\nPkg.add(\"Lux\")","category":"page"},{"location":"#Quick-Example","page":"Lux: Explicitly Parameterized Neural Networks","title":"Quick Example","text":"","category":"section"},{"location":"","page":"Lux: Explicitly Parameterized Neural Networks","title":"Lux: Explicitly Parameterized Neural Networks","text":"using Lux, Random, Optimisers, Zygote","category":"page"},{"location":"","page":"Lux: Explicitly Parameterized Neural Networks","title":"Lux: Explicitly Parameterized Neural Networks","text":"We take randomness very seriously","category":"page"},{"location":"","page":"Lux: Explicitly Parameterized Neural Networks","title":"Lux: Explicitly Parameterized Neural Networks","text":"# Seeding\nrng = Random.default_rng()\nRandom.seed!(rng, 0)","category":"page"},{"location":"","page":"Lux: Explicitly Parameterized Neural Networks","title":"Lux: Explicitly Parameterized Neural Networks","text":"Build the model","category":"page"},{"location":"","page":"Lux: Explicitly Parameterized Neural Networks","title":"Lux: Explicitly Parameterized Neural Networks","text":"# Construct the layer\nmodel = Chain(\n    BatchNorm(128),\n    Dense(128, 256, tanh),\n    BatchNorm(256),\n    Chain(\n        Dense(256, 1, tanh),\n        Dense(1, 10)\n    )\n)","category":"page"},{"location":"","page":"Lux: Explicitly Parameterized Neural Networks","title":"Lux: Explicitly Parameterized Neural Networks","text":"Models don't hold parameters and states so initialize them. From there on, we just use our standard AD and Optimisers API.","category":"page"},{"location":"","page":"Lux: Explicitly Parameterized Neural Networks","title":"Lux: Explicitly Parameterized Neural Networks","text":"# Parameter and State Variables\nps, st = Lux.setup(rng, model) .|> gpu\n\n# Dummy Input\nx = rand(rng, Float32, 128, 2) |> gpu\n\n# Run the model\ny, st = Lux.apply(model, x, ps, st)\n\n# Gradients\ngs = gradient(p -> sum(Lux.apply(model, x, p, st)[1]), ps)[1]\n\n# Optimization\nst_opt = Optimisers.setup(Optimisers.ADAM(0.0001), ps)\nst_opt, ps = Optimisers.update(st_opt, ps, gs)","category":"page"},{"location":"#Citation","page":"Lux: Explicitly Parameterized Neural Networks","title":"Citation","text":"","category":"section"},{"location":"","page":"Lux: Explicitly Parameterized Neural Networks","title":"Lux: Explicitly Parameterized Neural Networks","text":"If you found this library to be useful in academic work, then please cite:","category":"page"},{"location":"","page":"Lux: Explicitly Parameterized Neural Networks","title":"Lux: Explicitly Parameterized Neural Networks","text":"@misc{pal2022lux,\n    author = {Pal, Avik},\n    title = {Lux: Explicit Parameterization of Deep Neural Networks in Julia},\n    year = {2022},\n    publisher = {GitHub},\n    journal = {GitHub repository},\n    howpublished = {\\url{https://github.com/avik-pal/Lux.jl/}}\n}","category":"page"},{"location":"","page":"Lux: Explicitly Parameterized Neural Networks","title":"Lux: Explicitly Parameterized Neural Networks","text":"Also consider starring our github repo","category":"page"}]
}
