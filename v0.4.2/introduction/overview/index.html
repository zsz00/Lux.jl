<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>All about Lux · Lux</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.044/juliamono.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/custom.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Lux</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Lux: Explicitly Parameterized Neural Networks</a></li><li><span class="tocitem">Introduction</span><ul><li class="is-active"><a class="tocitem" href>All about Lux</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#Design-Principles"><span>Design Principles</span></a></li><li class="toplevel"><a class="tocitem" href="#AbstractExplicitLayer-API"><span><code>AbstractExplicitLayer</code> API</span></a></li><li class="toplevel"><a class="tocitem" href="#Why-use-Lux-over-Flux?"><span>Why use Lux over Flux?</span></a></li></ul></li><li><a class="tocitem" href="../ecosystem/">Ecosystem</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><input class="collapse-toggle" id="menuitem-3-1" type="checkbox"/><label class="tocitem" for="menuitem-3-1"><span class="docs-label">Beginner</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../examples/generated/beginner/Basics/main/">Julia &amp; Lux for the Uninitiated</a></li><li><a class="tocitem" href="../../examples/generated/beginner/SimpleRNN/main/">Training a Simple LSTM</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3-2" type="checkbox"/><label class="tocitem" for="menuitem-3-2"><span class="docs-label">Intermediate</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../examples/generated/intermediate/NeuralODE/main/">MNIST NeuralODE Classification</a></li><li><a class="tocitem" href="../../examples/generated/intermediate/BayesianNN/main/">Bayesian Neural Network</a></li></ul></li><li><span class="tocitem">Advanced</span></li><li><a class="tocitem" href="../../examples/">Additional Examples</a></li></ul></li><li><span class="tocitem">API</span><ul><li><a class="tocitem" href="../../api/layers/">Layers</a></li><li><a class="tocitem" href="../../api/functional/">Functional</a></li><li><a class="tocitem" href="../../api/core/">Core</a></li><li><a class="tocitem" href="../../api/utilities/">Utilities</a></li></ul></li><li><span class="tocitem">Design Docs</span><ul><li><a class="tocitem" href="../../design/documentation/">Documentation</a></li><li><a class="tocitem" href="../../design/recurrent/">Recurrent Neural Networks</a></li><li><a class="tocitem" href="../../design/core/">Add new functionality to Lux</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Introduction</a></li><li class="is-active"><a href>All about Lux</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>All about Lux</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/avik-pal/Lux.jl/blob/master/docs/src/introduction/overview.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Why-we-wrote-Lux?"><a class="docs-heading-anchor" href="#Why-we-wrote-Lux?">Why we wrote Lux?</a><a id="Why-we-wrote-Lux?-1"></a><a class="docs-heading-anchor-permalink" href="#Why-we-wrote-Lux?" title="Permalink"></a></h1><p>Julia already has quite a few well established Neural Network Frameworks – <a href="https://fluxml.ai/">Flux</a> &amp; <a href="https://denizyuret.github.io/Knet.jl/latest/">KNet</a>. However, certain design elements – <strong>Coupled Model and Parameters</strong> &amp; <strong>Internal Mutations</strong> – associated with these frameworks make them less compiler and user friendly. Making changes to address these problems in the respective frameworks would be too disruptive for users. Here comes in <code>Lux</code> a neural network framework built completely using pure functions to make it both compiler and autodiff friendly.</p><h1 id="Design-Principles"><a class="docs-heading-anchor" href="#Design-Principles">Design Principles</a><a id="Design-Principles-1"></a><a class="docs-heading-anchor-permalink" href="#Design-Principles" title="Permalink"></a></h1><ul><li><strong>Layers must be immutable</strong> – cannot store any parameter/state but rather store the information to construct them</li><li><strong>Layers are pure functions</strong></li><li><strong>Layers return a Tuple containing the result and the updated state</strong></li><li><strong>Given same inputs the outputs must be same</strong> – yes this must hold true even for stochastic functions. Randomness must be controlled using <code>rng</code>s passed in the state.</li><li><strong>Easily extendible</strong></li></ul><h1 id="AbstractExplicitLayer-API"><a class="docs-heading-anchor" href="#AbstractExplicitLayer-API"><code>AbstractExplicitLayer</code> API</a><a id="AbstractExplicitLayer-API-1"></a><a class="docs-heading-anchor-permalink" href="#AbstractExplicitLayer-API" title="Permalink"></a></h1><p>We provide 2 abstract layers:</p><ol><li><p><code>AbstractExplicitLayer</code>: Useful for Base Layers and needs to define the following functions</p><ul><li><code>initialparameters(rng::AbstractRNG, layer::CustomAbstractExplicitLayer)</code> – This returns a <code>ComponentArray</code>/<code>NamedTuple</code> containing the trainable parameters for the layer.</li><li><code>initialstates(rng::AbstractRNG, layer::CustomAbstractExplicitLayer)</code> – This returns a NamedTuple containing the current state for the layer. For most layers this is typically empty. Layers that would potentially contain this include <code>BatchNorm</code>, <code>LSTM</code>, <code>GRU</code> etc.</li><li><code>parameterlength(layer::CustomAbstractExplicitLayer)</code> – These can be automatically calculated, but it is recommended that the user defines these.</li><li><code>statelength(layer::CustomAbstractExplicitLayer)</code> – These can be automatically calculated, but it is recommended that the user defines these.</li></ul></li><li><p><code>AbstractExplicitContainerLayer</code>: Used when the layer is storing other <code>AbstractExplicitLayer</code>s or <code>AbstractExplicitContainerLayer</code>s. This allows good defaults of the dispatches for functions mentioned in the previous point.</p></li></ol><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>We recommend users to subtype their custom layers using <code>AbstractExplicitLayer</code> or <code>AbstractExplicitContainerLayer</code>. However, this is <strong>not mandatory</strong>.</p></div></div><h1 id="Why-use-Lux-over-Flux?"><a class="docs-heading-anchor" href="#Why-use-Lux-over-Flux?">Why use Lux over Flux?</a><a id="Why-use-Lux-over-Flux?-1"></a><a class="docs-heading-anchor-permalink" href="#Why-use-Lux-over-Flux?" title="Permalink"></a></h1><ul><li><strong>Large Neural Networks</strong><ul><li>For small neural networks we recommend <a href="https://github.com/PumasAI/SimpleChains.jl">SimpleChains.jl</a>.</li><li>For SciML Applications (Neural ODEs, Deep Equilibrium Models) solvers typically expect a monolithic parameter vector. Flux enables this via its <code>destructure</code> mechanism, however, it often leads to <a href="https://github.com/FluxML/Flux.jl/issues?q=is%3Aissue+destructure">weird bugs</a>. Lux forces users to make an explicit distinction between state variables and parameter variables to avoid these issues.</li><li>Comes battery-included for distributed training using <a href="https://github.com/avik-pal/FluxMPI.jl">FluxMPI.jl</a></li></ul></li><li><strong>Sensible display of Custom Layers</strong> – Ever wanted to see Pytorch like Network printouts or wondered how to extend the pretty printing of Flux&#39;s layers. Lux handles all of that by default.</li><li><strong>Less Bug-ridden Code</strong><ul><li><em>No arbitrary internal mutations</em> – all layers are implemented as pure functions.</li><li><em>All layers are deterministic</em> given the parameter and state – if the layer is supposed to be stochastic (say <code>Dropout</code>), the state must contain a seed which is then updated after the function call.</li></ul></li><li><strong>Easy Parameter Manipulation</strong> – Wondering why Flux doesn&#39;t have <code>WeightNorm</code>, <code>SpectralNorm</code>, etc. The implicit parameter handling makes it extremely hard to pass parameters around without mutations which AD systems don&#39;t like. With Lux implementing them is outright simple.</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../">« Lux: Explicitly Parameterized Neural Networks</a><a class="docs-footer-nextpage" href="../ecosystem/">Ecosystem »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.18 on <span class="colophon-date" title="Wednesday 25 May 2022 23:31">Wednesday 25 May 2022</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
