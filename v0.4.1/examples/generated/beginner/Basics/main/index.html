<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Julia &amp; Lux for the Uninitiated · Lux</title><script async src="https://www.googletagmanager.com/gtag/js?id=G-Q8GYTEVTZ2"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-Q8GYTEVTZ2', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../../../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.044/juliamono.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../../../../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../../../../assets/documenter.js"></script><script src="../../../../../siteinfo.js"></script><script src="../../../../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../../../../assets/themeswap.js"></script><link href="../../../../../assets/custom.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../../../../">Lux</a></span></div><form class="docs-search" action="../../../../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../../../../">Lux: Explicitly Parameterized Neural Networks</a></li><li><span class="tocitem">Introduction</span><ul><li><a class="tocitem" href="../../../../../introduction/overview/">All about Lux</a></li><li><a class="tocitem" href="../../../../../introduction/ecosystem/">Ecosystem</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><input class="collapse-toggle" id="menuitem-3-1" type="checkbox" checked/><label class="tocitem" for="menuitem-3-1"><span class="docs-label">Beginner</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>Julia &amp; Lux for the Uninitiated</a><ul class="internal"><li><a class="tocitem" href="#Arrays"><span>Arrays</span></a></li><li><a class="tocitem" href="#(Im)mutability"><span>(Im)mutability</span></a></li><li><a class="tocitem" href="#Managing-Randomness"><span>Managing Randomness</span></a></li><li><a class="tocitem" href="#Automatic-Differentiation"><span>Automatic Differentiation</span></a></li><li><a class="tocitem" href="#Linear-Regression"><span>Linear Regression</span></a></li></ul></li><li><a class="tocitem" href="../../SimpleRNN/main/">Training a Simple LSTM</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3-2" type="checkbox"/><label class="tocitem" for="menuitem-3-2"><span class="docs-label">Intermediate</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../../intermediate/NeuralODE/main/">MNIST NeuralODE Classification</a></li><li><a class="tocitem" href="../../../intermediate/BayesianNN/main/">Bayesian Neural Network</a></li></ul></li><li><span class="tocitem">Advanced</span></li><li><a class="tocitem" href="../../../../">Additional Examples</a></li></ul></li><li><span class="tocitem">API</span><ul><li><a class="tocitem" href="../../../../../api/layers/">Layers</a></li><li><a class="tocitem" href="../../../../../api/functional/">Functional</a></li><li><a class="tocitem" href="../../../../../api/core/">Core</a></li><li><a class="tocitem" href="../../../../../api/utilities/">Utilities</a></li></ul></li><li><span class="tocitem">Design Docs</span><ul><li><a class="tocitem" href="../../../../../design/documentation/">Documentation</a></li><li><a class="tocitem" href="../../../../../design/recurrent/">Recurrent Neural Networks</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li><a class="is-disabled">Beginner</a></li><li class="is-active"><a href>Julia &amp; Lux for the Uninitiated</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Julia &amp; Lux for the Uninitiated</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/avik-pal/Lux.jl/blob/main/examples/Basics/main.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Julia-and-Lux-for-the-Uninitiated"><a class="docs-heading-anchor" href="#Julia-and-Lux-for-the-Uninitiated">Julia &amp; Lux for the Uninitiated</a><a id="Julia-and-Lux-for-the-Uninitiated-1"></a><a class="docs-heading-anchor-permalink" href="#Julia-and-Lux-for-the-Uninitiated" title="Permalink"></a></h1><p>This is a quick intro to <a href="https://github.com/avik-pal/:ux.jl">Lux</a> loosely based on:</p><ol><li><a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html">PyTorch&#39;s tutorial</a>.</li><li><a href="https://fluxml.ai/tutorials/2020/09/15/deep-learning-flux.html">Flux&#39;s tutorial</a>.</li><li><a href="https://flax.readthedocs.io/en/latest/notebooks/jax_for_the_impatient.html">Flax&#39;s tutorial</a>.</li></ol><p>It introduces basic Julia programming, as well <code>Zygote</code>, a source-to-source automatic differentiation (AD) framework in Julia. We&#39;ll use these tools to build a very simple neural network. Let&#39;s start with importing <code>Lux.jl</code></p><pre><code class="language-julia hljs">using Lux, Random</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">  Activating project at `~/work/Lux.jl/Lux.jl/examples`</code></pre><p>Now let us control the randomness in our code using proper Pseudo Random Number Generator (PRNG)</p><pre><code class="language-julia hljs">rng = Random.default_rng()
Random.seed!(rng, 0)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Random.TaskLocalRNG()</code></pre><h2 id="Arrays"><a class="docs-heading-anchor" href="#Arrays">Arrays</a><a id="Arrays-1"></a><a class="docs-heading-anchor-permalink" href="#Arrays" title="Permalink"></a></h2><p>The starting point for all of our models is the <code>Array</code> (sometimes referred to as a <code>Tensor</code> in other frameworks). This is really just a list of numbers, which might be arranged into a shape like a square. Let&#39;s write down an array with three elements.</p><pre><code class="language-julia hljs">x = [1, 2, 3]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">3-element Vector{Int64}:
 1
 2
 3</code></pre><p>Here&#39;s a matrix – a square array with four elements.</p><pre><code class="language-julia hljs">x = [1 2; 3 4]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2×2 Matrix{Int64}:
 1  2
 3  4</code></pre><p>We often work with arrays of thousands of elements, and don&#39;t usually write them down by hand. Here&#39;s how we can create an array of 5×3 = 15 elements, each a random number from zero to one.</p><pre><code class="language-julia hljs">x = rand(rng, 5, 3)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5×3 Matrix{Float64}:
 0.455238   0.746943   0.193291
 0.547642   0.746801   0.116989
 0.773354   0.97667    0.899766
 0.940585   0.0869468  0.422918
 0.0296477  0.351491   0.707534</code></pre><p>There&#39;s a few functions like this; try replacing <code>rand</code> with <code>ones</code>, <code>zeros</code>, or <code>randn</code>.</p><p>By default, Julia works stores numbers is a high-precision format called <code>Float64</code>. In ML we often don&#39;t need all those digits, and can ask Julia to work with <code>Float32</code> instead. We can even ask for more digits using <code>BigFloat</code>.</p><pre><code class="language-julia hljs">x = rand(BigFloat, 5, 3)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5×3 Matrix{BigFloat}:
 0.981339    0.793159  0.459019
 0.043883    0.624384  0.56055
 0.164786    0.524008  0.0355555
 0.414769    0.577181  0.621958
 0.00823197  0.30215   0.655881</code></pre><pre><code class="language-julia hljs">x = rand(Float32, 5, 3)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5×3 Matrix{Float32}:
 0.567794   0.369178   0.342539
 0.0985227  0.201145   0.587206
 0.776598   0.148248   0.0851708
 0.723731   0.0770206  0.839303
 0.404728   0.230954   0.679087</code></pre><p>We can ask the array how many elements it has.</p><pre><code class="language-julia hljs">length(x)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">15</code></pre><p>Or, more specifically, what size it has.</p><pre><code class="language-julia hljs">size(x)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(5, 3)</code></pre><p>We sometimes want to see some elements of the array on their own.</p><pre><code class="language-julia hljs">x</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5×3 Matrix{Float32}:
 0.567794   0.369178   0.342539
 0.0985227  0.201145   0.587206
 0.776598   0.148248   0.0851708
 0.723731   0.0770206  0.839303
 0.404728   0.230954   0.679087</code></pre><pre><code class="language-julia hljs">x[2, 3]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.58720636f0</code></pre><p>This means get the second row and the third column. We can also get every row of the third column.</p><pre><code class="language-julia hljs">x[:, 3]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5-element Vector{Float32}:
 0.34253937
 0.58720636
 0.085170805
 0.8393034
 0.67908657</code></pre><p>We can add arrays, and subtract them, which adds or subtracts each element of the array.</p><pre><code class="language-julia hljs">x + x</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5×3 Matrix{Float32}:
 1.13559   0.738356  0.685079
 0.197045  0.40229   1.17441
 1.5532    0.296496  0.170342
 1.44746   0.154041  1.67861
 0.809456  0.461908  1.35817</code></pre><pre><code class="language-julia hljs">x - x</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5×3 Matrix{Float32}:
 0.0  0.0  0.0
 0.0  0.0  0.0
 0.0  0.0  0.0
 0.0  0.0  0.0
 0.0  0.0  0.0</code></pre><p>Julia supports a feature called <em>broadcasting</em>, using the <code>.</code> syntax. This tiles small arrays (or single numbers) to fill bigger ones.</p><pre><code class="language-julia hljs">x .+ 1</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5×3 Matrix{Float32}:
 1.56779  1.36918  1.34254
 1.09852  1.20114  1.58721
 1.7766   1.14825  1.08517
 1.72373  1.07702  1.8393
 1.40473  1.23095  1.67909</code></pre><p>We can see Julia tile the column vector <code>1:5</code> across all rows of the larger array.</p><pre><code class="language-julia hljs">zeros(5,5) .+ (1:5)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5×5 Matrix{Float64}:
 1.0  1.0  1.0  1.0  1.0
 2.0  2.0  2.0  2.0  2.0
 3.0  3.0  3.0  3.0  3.0
 4.0  4.0  4.0  4.0  4.0
 5.0  5.0  5.0  5.0  5.0</code></pre><p>The x&#39; syntax is used to transpose a column <code>1:5</code> into an equivalent row, and Julia will tile that across columns.</p><pre><code class="language-julia hljs">zeros(5,5) .+ (1:5)&#39;</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5×5 Matrix{Float64}:
 1.0  2.0  3.0  4.0  5.0
 1.0  2.0  3.0  4.0  5.0
 1.0  2.0  3.0  4.0  5.0
 1.0  2.0  3.0  4.0  5.0
 1.0  2.0  3.0  4.0  5.0</code></pre><p>We can use this to make a times table.</p><pre><code class="language-julia hljs">(1:5) .* (1:5)&#39;</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5×5 Matrix{Int64}:
 1   2   3   4   5
 2   4   6   8  10
 3   6   9  12  15
 4   8  12  16  20
 5  10  15  20  25</code></pre><p>Finally, and importantly for machine learning, we can conveniently do things like matrix multiply.</p><pre><code class="language-julia hljs">W = randn(5, 10)
x = rand(10)
W * x</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5-element Vector{Float64}:
  1.2197981041108443
 -2.6262587710059595
 -2.8573820474674845
 -2.4319346874291305
  1.010866857715021</code></pre><p>Julia&#39;s arrays are very powerful, and you can learn more about what they can do <a href="https://docs.julialang.org/en/v1/manual/arrays/">here</a>.</p><h3 id="CUDA-Arrays"><a class="docs-heading-anchor" href="#CUDA-Arrays">CUDA Arrays</a><a id="CUDA-Arrays-1"></a><a class="docs-heading-anchor-permalink" href="#CUDA-Arrays" title="Permalink"></a></h3><p>CUDA functionality is provided separately by the <a href="https://github.com/JuliaGPU/CUDA.jl">CUDA.jl package</a>. If you have a GPU and CUDA available, <code>Lux</code> will automatically build the required CUDA dependencies using <code>CUDA.jl</code>.</p><p>You can manually add <code>CUDA</code>. Once CUDA is loaded you can move any array to the GPU with the <code>cu</code> function (or the <code>gpu</code> function exported by <code>Lux</code>`), and it supports all of the above operations with the same syntax.</p><pre><code class="language-julia hljs"># using CUDA
# x = cu(rand(5, 3))</code></pre><h2 id="(Im)mutability"><a class="docs-heading-anchor" href="#(Im)mutability">(Im)mutability</a><a id="(Im)mutability-1"></a><a class="docs-heading-anchor-permalink" href="#(Im)mutability" title="Permalink"></a></h2><p>Lux as you might have read is <a href="http://lux.csail.mit.edu/dev/introduction/overview/#Design-Principles">Immutable by convention</a> which means that the core library is built without any form of mutation and all functions are pure. However, we don&#39;t enfore it in any form. We do <strong>strongly recommend</strong> that users extending this framework for their respective applications don&#39;t mutate their arrays.</p><pre><code class="language-julia hljs">x = reshape(1:8, 2, 4)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2×4 reshape(::UnitRange{Int64}, 2, 4) with eltype Int64:
 1  3  5  7
 2  4  6  8</code></pre><p>To update this array, we should first copy the array.</p><pre><code class="language-julia hljs">x_copy = copy(x)
view(x_copy, :, 1) .= 0

println(&quot;Original Array &quot;, x)
println(&quot;Mutated Array &quot;, x_copy)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Original Array [1 3 5 7; 2 4 6 8]
Mutated Array [0 3 5 7; 0 4 6 8]</code></pre><p>Note that our current default AD engine (Zygote) is unable to differentiate through this mutation, however, for these specialized cases it is quite trivial to write custom backward passes. (This problem will be fixed once we move towards Enzyme.jl)</p><h2 id="Managing-Randomness"><a class="docs-heading-anchor" href="#Managing-Randomness">Managing Randomness</a><a id="Managing-Randomness-1"></a><a class="docs-heading-anchor-permalink" href="#Managing-Randomness" title="Permalink"></a></h2><p>We relu on the Julia StdLib <code>Random</code> for managing the randomness in our execution. First, we create an PRNG and seed it.</p><pre><code class="language-julia hljs">rng = Random.default_rng() # Creates a Xoshiro PRNG
Random.seed!(rng, 0)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Random.TaskLocalRNG()</code></pre><p>If we call any function that relies on <code>rng</code> and uses it via <code>randn</code>, <code>rand</code>, etc. <code>rng</code> will be mutated. As we have already established we care a lot about immutability, hence we should use <code>Lux.replicate</code> on PRNG before using them.</p><p>First, let us run a random number generator 3 times with the <code>replicate</code>d rng</p><pre><code class="language-julia hljs">for i = 1:3
    println(&quot;Iteration $i &quot;, rand(Lux.replicate(rng), 10))
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Iteration 1 [0.4552384158732863, 0.5476424498276177, 0.7733535276924052, 0.9405848223512736, 0.02964765308691042, 0.74694291453392, 0.7468008914093891, 0.9766699015845924, 0.08694684883050086, 0.35149138733595564]
Iteration 2 [0.4552384158732863, 0.5476424498276177, 0.7733535276924052, 0.9405848223512736, 0.02964765308691042, 0.74694291453392, 0.7468008914093891, 0.9766699015845924, 0.08694684883050086, 0.35149138733595564]
Iteration 3 [0.4552384158732863, 0.5476424498276177, 0.7733535276924052, 0.9405848223512736, 0.02964765308691042, 0.74694291453392, 0.7468008914093891, 0.9766699015845924, 0.08694684883050086, 0.35149138733595564]</code></pre><p>As expected we get the same output. We can remove the <code>replicate</code> call and we will get different outputs</p><pre><code class="language-julia hljs">for i = 1:3
    println(&quot;Iteration $i &quot;, rand(rng, 10))
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Iteration 1 [0.4552384158732863, 0.5476424498276177, 0.7733535276924052, 0.9405848223512736, 0.02964765308691042, 0.74694291453392, 0.7468008914093891, 0.9766699015845924, 0.08694684883050086, 0.35149138733595564]
Iteration 2 [0.018743665453639813, 0.8601828553599953, 0.6556360448565952, 0.7746656838366666, 0.7817315740767116, 0.5553797706980106, 0.1261990389976131, 0.4488101521328277, 0.624383955429775, 0.05657739601024536]
Iteration 3 [0.19597391412112541, 0.6830945313415872, 0.6776220912718907, 0.6456416023530093, 0.6340362477836592, 0.5595843665394066, 0.5675557670686644, 0.34351700231383653, 0.7237308297251812, 0.3691778381831775]</code></pre><h2 id="Automatic-Differentiation"><a class="docs-heading-anchor" href="#Automatic-Differentiation">Automatic Differentiation</a><a id="Automatic-Differentiation-1"></a><a class="docs-heading-anchor-permalink" href="#Automatic-Differentiation" title="Permalink"></a></h2><p>Julia has quite a few (maybe too many) AD tools. For the purpose of this tutorial, we will use <a href="https://github.com/JuliaDiff/AbstractDifferentiation.jl">AbstractDifferentiation.jl</a> which provides an uniform API across multiple AD backends. For the backends we will use:</p><ol><li><a href="https://github.com/JuliaDiff/ForwardDiff.jl">ForwardDiff.jl</a> – For Jacobian-Vector Product (JVP)</li><li><a href="https://github.com/FluxML/Zygote.jl">Zygote.jl</a> – For Vector-Jacobian Product (VJP)</li></ol><p><em>Slight Detour</em>: We have had several questions regarding if we will be considering any other AD system for the reverse-diff backend. For now we will stick to Zygote.jl, however once <a href="https://github.com/EnzymeAD/Enzyme.jl">Enzyme.jl</a> has support for custom rules and we have tested Lux extensively with it, we will make the switch.</p><p>Even though, theoretically, a VJP (Vector-Jacobian product - reverse autodiff) and a JVP (Jacobian-Vector product - forward-mode autodiff) are similar—they compute a product of a Jacobian and a vector—they differ by the computational complexity of the operation. In short, when you have a large number of parameters (hence a wide matrix), a JVP is less efficient computationally than a VJP, and, conversely, a JVP is more efficient when the Jacobian matrix is a tall matrix.</p><pre><code class="language-julia hljs">using ForwardDiff, Zygote, AbstractDifferentiation</code></pre><h3 id="Gradients"><a class="docs-heading-anchor" href="#Gradients">Gradients</a><a id="Gradients-1"></a><a class="docs-heading-anchor-permalink" href="#Gradients" title="Permalink"></a></h3><p>For our first example, consider a simple function computing <span>$f(x) = \frac{1}{2}x^T x$</span>, where <span>$\nabla f(x) = x$</span></p><pre><code class="language-julia hljs">f(x) = x&#39; * x / 2
∇f(x) = x
v = randn(rng, Float32, 4)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">4-element Vector{Float32}:
 -0.4051151
 -0.4593922
  0.92155594
  1.1871622</code></pre><p>Let&#39;s use AbstractDifferentiation and Zygote to compute the gradients</p><pre><code class="language-julia hljs">println(&quot;Actual Gradient: &quot;, ∇f(v))
println(&quot;Computed Gradient via Reverse Mode AD (Zygote): &quot;, AD.gradient(AD.ZygoteBackend(), f, v)[1])
println(&quot;Computed Gradient via Forward Mode AD (ForwardDiff): &quot;, AD.gradient(AD.ForwardDiffBackend(), f, v)[1])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Actual Gradient: Float32[-0.4051151, -0.4593922, 0.92155594, 1.1871622]
Computed Gradient via Reverse Mode AD (Zygote): Float32[-0.4051151, -0.4593922, 0.92155594, 1.1871622]
Computed Gradient via Forward Mode AD (ForwardDiff): Float32[-0.4051151, -0.4593922, 0.92155594, 1.1871622]</code></pre><p>Note that <code>AD.gradient</code> will only work for scalar valued outputs</p><h3 id="Jacobian-Vector-Product"><a class="docs-heading-anchor" href="#Jacobian-Vector-Product">Jacobian-Vector Product</a><a id="Jacobian-Vector-Product-1"></a><a class="docs-heading-anchor-permalink" href="#Jacobian-Vector-Product" title="Permalink"></a></h3><p>I will defer the discussion on forward-mode AD to https://book.sciml.ai/notes/08/. Here let us just look at a mini example on how to use it.</p><pre><code class="language-julia hljs">f(x) = x .* x ./ 2
x = randn(rng, Float32, 5)
v = ones(Float32, 5)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5-element Vector{Float32}:
 1.0
 1.0
 1.0
 1.0
 1.0</code></pre><p>Construct the pushforward function.</p><pre><code class="language-julia hljs">pf_f = AD.value_and_pushforward_function(AD.ForwardDiffBackend(), f, x)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">#17 (generic function with 1 method)</code></pre><p>Compute the jvp</p><pre><code class="language-julia hljs">val, jvp = pf_f(v)
println(&quot;Computed Value: f(&quot;, x, &quot;) = &quot;, val)
println(&quot;JVP: &quot;, jvp[1])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Computed Value: f(Float32[-0.877497, 1.1953009, -0.057005208, 0.25055695, 0.09351656]) = Float32[0.3850005, 0.71437216, 0.0016247969, 0.031389393, 0.0043726736]
JVP: Float32[-0.877497, 1.1953009, -0.057005208, 0.25055695, 0.09351656]</code></pre><h3 id="Vector-Jacobian-Product"><a class="docs-heading-anchor" href="#Vector-Jacobian-Product">Vector-Jacobian Product</a><a id="Vector-Jacobian-Product-1"></a><a class="docs-heading-anchor-permalink" href="#Vector-Jacobian-Product" title="Permalink"></a></h3><p>Using the same function and inputs, let us compute the VJP</p><pre><code class="language-julia hljs">pb_f = AD.value_and_pullback_function(AD.ZygoteBackend(), f, x)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">#25 (generic function with 1 method)</code></pre><p>Compute the vjp</p><pre><code class="language-julia hljs">val, vjp = pb_f(v)
println(&quot;Computed Value: f(&quot;, x, &quot;) = &quot;, val)
println(&quot;VJP: &quot;, vjp[1])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Computed Value: f(Float32[-0.877497, 1.1953009, -0.057005208, 0.25055695, 0.09351656]) = Float32[0.3850005, 0.71437216, 0.0016247969, 0.031389393, 0.0043726736]
VJP: Float32[-0.877497, 1.1953009, -0.057005208, 0.25055695, 0.09351656]</code></pre><h2 id="Linear-Regression"><a class="docs-heading-anchor" href="#Linear-Regression">Linear Regression</a><a id="Linear-Regression-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-Regression" title="Permalink"></a></h2><p>Finally, now let us consider a linear regression problem. From a set of data-points <span>$\left\{ (x_i, y_i), i \in \left\{ 1, \dots, k \right\}, x_i \in \R^n, y_i \in \R^m \right\}$</span>, we try to find a set of parameters <span>$W$</span> and <span>$b$</span>, s.t. <span>$f_{W,b}(x) = Wx + b$</span> minimizes the mean squared error:</p><p class="math-container">\[L(W, b) \longrightarrow \sum_{i = 1}^{k} \frac{1}{2} \| y_i - f_{W,b}(x_i) \|_2^2\]</p><p>We can write <code>f</code> from scratch, but to demonstrate <code>Lux</code> let us use the <code>Dense</code> layer.</p><pre><code class="language-julia hljs">model = Dense(10 =&gt; 5)

rng = Random.default_rng()
Random.seed!(rng, 0)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Random.TaskLocalRNG()</code></pre><p>Let us initialize the parameters and states (in this case it is empty) for the model</p><pre><code class="language- hljs">ps, st = Lux.setup(rng, model)
ps = ps |&gt; ComponentArray</code></pre><p>Set problem dimensions.</p><pre><code class="language-julia hljs">n_samples = 20
x_dim = 10
y_dim = 5</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5</code></pre><p>Generate random ground truth W and b.</p><pre><code class="language-julia hljs">W = randn(rng, Float32, y_dim, x_dim)
b = randn(rng, Float32, y_dim)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">5-element Vector{Float32}:
  0.68468636
 -0.57578707
  0.0594993
 -0.9436797
  1.5164032</code></pre><p>Generate samples with additional noise.</p><pre><code class="language-julia hljs">x_samples = randn(rng, Float32, x_dim, n_samples)
y_samples = W * x_samples .+ b .+ 0.01f0 .* randn(rng, Float32, y_dim, n_samples)
println(&quot;x shape: &quot;, size(x_samples), &quot;; y shape: &quot;, size(y_samples))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">x shape: (10, 20); y shape: (5, 20)</code></pre><p>For updating our parameters let&#39;s use <a href="https://github.com/FluxML/Optimisers.jl">Optimisers.jl</a></p><pre><code class="language-julia hljs">using Optimisers

opt = Optimisers.Descent(0.01f0)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Optimisers.Descent{Float32}(0.01f0)</code></pre><p>Initialize the initial state of the optimiser</p><pre><code class="language-julia hljs">opt_state = Optimisers.setup(opt, ps)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(weight = Leaf(Descent{Float32}(0.01), nothing), bias = Leaf(Descent{Float32}(0.01), nothing))</code></pre><p>Define the loss function</p><pre><code class="language-julia hljs">mse(model, ps, st, X, y) = sum(abs2, model(X, ps, st)[1] .- y)
mse(weight, bias, X, y) = sum(abs2, weight * X .+ bias .- y)
loss_function(ps, X, y) = mse(model, ps, st, X, y)

println(&quot;Loss Value with ground true W &amp; b: &quot;, mse(W, b, x_samples, y_samples))

for i in 1:100
    # In actual code, don&#39;t use globals. But here I will simply for the sake of demonstration
    global ps, st, opt_state
    # Compute the gradient
    gs = gradient(loss_function, ps, x_samples, y_samples)[1]
    # Perform parameter update
    opt_state, ps = Optimisers.update(opt_state, ps, gs)
    if i % 10 == 1 || i == 100
        println(&quot;Loss Value after $i iterations: &quot;, mse(model, ps, st, x_samples, y_samples))
    end
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Loss Value with ground true W &amp; b: 0.009175307
Loss Value after 1 iterations: 165.57005
Loss Value after 11 iterations: 4.351237
Loss Value after 21 iterations: 0.6856849
Loss Value after 31 iterations: 0.15421417
Loss Value after 41 iterations: 0.041469414
Loss Value after 51 iterations: 0.014032223
Loss Value after 61 iterations: 0.006883738
Loss Value after 71 iterations: 0.004938521
Loss Value after 81 iterations: 0.004391277
Loss Value after 91 iterations: 0.0042331247
Loss Value after 100 iterations: 0.0041888584</code></pre><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../../../../introduction/ecosystem/">« Ecosystem</a><a class="docs-footer-nextpage" href="../../SimpleRNN/main/">Training a Simple LSTM »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.17 on <span class="colophon-date" title="Tuesday 24 May 2022 05:37">Tuesday 24 May 2022</span>. Using Julia version 1.7.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
